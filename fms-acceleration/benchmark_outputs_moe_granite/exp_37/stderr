/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|                                        | 0/6 [00:00<?, ?it/s]You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|                                        | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█████▎                          | 1/6 [00:04<00:20,  4.01s/it]Loading checkpoint shards:  33%|██████████▋                     | 2/6 [00:12<00:26,  6.73s/it]Loading checkpoint shards:  50%|████████████████                | 3/6 [00:21<00:23,  7.71s/it]Loading checkpoint shards:  67%|█████████████████████▎          | 4/6 [00:30<00:16,  8.15s/it]Loading checkpoint shards:  83%|██████████████████████████▋     | 5/6 [00:38<00:08,  8.00s/it]Loading checkpoint shards: 100%|████████████████████████████████| 6/6 [00:43<00:00,  6.96s/it]Loading checkpoint shards: 100%|████████████████████████████████| 6/6 [00:43<00:00,  7.17s/it]
WARNING:sft_trainer.py:PAD token set to default, missing in tokenizer
Loading checkpoint shards:  17%|█████▎                          | 1/6 [00:57<04:47, 57.47s/it]Loading checkpoint shards:  33%|██████████▋                     | 2/6 [01:40<03:16, 49.13s/it]Loading checkpoint shards:  50%|████████████████                | 3/6 [02:19<02:13, 44.46s/it]Loading checkpoint shards:  67%|█████████████████████▎          | 4/6 [02:56<01:23, 41.51s/it]Loading checkpoint shards:  83%|██████████████████████████▋     | 5/6 [03:39<00:41, 41.95s/it]Loading checkpoint shards: 100%|████████████████████████████████| 6/6 [03:50<00:00, 31.40s/it]Loading checkpoint shards: 100%|████████████████████████████████| 6/6 [03:50<00:00, 38.38s/it]
WARNING:sft_trainer.py:PAD token set to default, missing in tokenizer
You are using a model of type granitemoeshared to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
Converting ScatterMoE layers:   0%|                                    | 0/40 [00:00<?, ?it/s]You are using a model of type granitemoeshared to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
Converting ScatterMoE layers:   2%|▋                           | 1/40 [00:13<08:52, 13.66s/it]Converting ScatterMoE layers:   5%|█▍                          | 2/40 [00:19<05:36,  8.85s/it]Converting ScatterMoE layers:   8%|██                          | 3/40 [00:24<04:33,  7.40s/it]Converting ScatterMoE layers:  10%|██▊                         | 4/40 [00:30<03:55,  6.54s/it]Converting ScatterMoE layers:  12%|███▌                        | 5/40 [00:35<03:39,  6.27s/it]Converting ScatterMoE layers:  15%|████▏                       | 6/40 [00:41<03:24,  6.00s/it]Converting ScatterMoE layers:  18%|████▉                       | 7/40 [00:46<03:09,  5.74s/it]Converting ScatterMoE layers:  20%|█████▌                      | 8/40 [00:53<03:12,  6.02s/it]Converting ScatterMoE layers:  22%|██████▎                     | 9/40 [00:58<02:58,  5.75s/it]Converting ScatterMoE layers:  25%|██████▊                    | 10/40 [01:03<02:49,  5.64s/it]Converting ScatterMoE layers:  28%|███████▍                   | 11/40 [01:09<02:40,  5.54s/it]Converting ScatterMoE layers:  30%|████████                   | 12/40 [01:14<02:33,  5.48s/it]Converting ScatterMoE layers:  32%|████████▊                  | 13/40 [01:19<02:24,  5.35s/it]Converting ScatterMoE layers:  35%|█████████▍                 | 14/40 [01:24<02:19,  5.35s/it]Converting ScatterMoE layers:  38%|██████████▏                | 15/40 [01:31<02:23,  5.75s/it]Converting ScatterMoE layers:  40%|██████████▊                | 16/40 [01:37<02:19,  5.83s/it]Converting ScatterMoE layers:  42%|███████████▍               | 17/40 [01:42<02:09,  5.64s/it]Converting ScatterMoE layers:  45%|████████████▏              | 18/40 [01:47<02:01,  5.51s/it]Converting ScatterMoE layers:  48%|████████████▊              | 19/40 [01:52<01:53,  5.40s/it]Converting ScatterMoE layers:  50%|█████████████▌             | 20/40 [01:58<01:50,  5.55s/it]Converting ScatterMoE layers:  52%|██████████████▏            | 21/40 [02:04<01:44,  5.52s/it]Converting ScatterMoE layers:  55%|██████████████▊            | 22/40 [02:09<01:37,  5.40s/it]Converting ScatterMoE layers:  57%|███████████████▌           | 23/40 [02:15<01:37,  5.72s/it]Converting ScatterMoE layers:  60%|████████████████▏          | 24/40 [02:21<01:29,  5.61s/it]Converting ScatterMoE layers:  62%|████████████████▉          | 25/40 [02:26<01:24,  5.64s/it]Converting ScatterMoE layers:  65%|█████████████████▌         | 26/40 [02:32<01:17,  5.56s/it]Converting ScatterMoE layers:  68%|██████████████████▏        | 27/40 [02:37<01:12,  5.55s/it]Converting ScatterMoE layers:  70%|██████████████████▉        | 28/40 [02:43<01:06,  5.52s/it]Converting ScatterMoE layers:  72%|███████████████████▌       | 29/40 [02:48<01:00,  5.53s/it]Converting ScatterMoE layers:  75%|████████████████████▎      | 30/40 [02:54<00:56,  5.67s/it]Converting ScatterMoE layers:  78%|████████████████████▉      | 31/40 [03:02<00:55,  6.12s/it]Converting ScatterMoE layers:  80%|█████████████████████▌     | 32/40 [03:08<00:49,  6.15s/it]Converting ScatterMoE layers:  82%|██████████████████████▎    | 33/40 [03:13<00:41,  5.98s/it]Converting ScatterMoE layers:  85%|██████████████████████▉    | 34/40 [03:19<00:35,  5.91s/it]Converting ScatterMoE layers:  88%|███████████████████████▋   | 35/40 [03:24<00:28,  5.74s/it]Converting ScatterMoE layers:  90%|████████████████████████▎  | 36/40 [03:30<00:22,  5.63s/it]Converting ScatterMoE layers:  92%|████████████████████████▉  | 37/40 [03:35<00:16,  5.61s/it]Converting ScatterMoE layers:  95%|█████████████████████████▋ | 38/40 [03:40<00:10,  5.35s/it]Converting ScatterMoE layers:  98%|██████████████████████████▎| 39/40 [03:47<00:05,  5.73s/it]Converting ScatterMoE layers: 100%|███████████████████████████| 40/40 [03:52<00:00,  5.56s/it]Converting ScatterMoE layers: 100%|███████████████████████████| 40/40 [03:52<00:00,  5.81s/it]
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/tuning/sft_trainer.py:371: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  trainer = SFTTrainer(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/tuning/sft_trainer.py:371: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  trainer = SFTTrainer(
  0%|                                                                 | 0/100 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
  1%|▌                                                        | 1/100 [00:21<35:16, 21.37s/it]                                                                                                1%|▌                                                        | 1/100 [00:21<35:16, 21.37s/it]  2%|█▏                                                       | 2/100 [00:26<19:38, 12.02s/it]                                                                                                2%|█▏                                                       | 2/100 [00:26<19:38, 12.02s/it]  3%|█▋                                                       | 3/100 [00:32<14:35,  9.03s/it]                                                                                                3%|█▋                                                       | 3/100 [00:32<14:35,  9.03s/it]  4%|██▎                                                      | 4/100 [00:37<12:15,  7.66s/it]                                                                                                4%|██▎                                                      | 4/100 [00:37<12:15,  7.66s/it]  5%|██▊                                                      | 5/100 [00:43<10:47,  6.82s/it]                                                                                                5%|██▊                                                      | 5/100 [00:43<10:47,  6.82s/it]  6%|███▍                                                     | 6/100 [00:48<09:54,  6.33s/it]                                                                                                6%|███▍                                                     | 6/100 [00:48<09:54,  6.33s/it]  7%|███▉                                                     | 7/100 [00:53<09:15,  5.97s/it]                                                                                                7%|███▉                                                     | 7/100 [00:53<09:15,  5.97s/it]  8%|████▌                                                    | 8/100 [00:59<08:53,  5.80s/it]                                                                                                8%|████▌                                                    | 8/100 [00:59<08:53,  5.80s/it]  9%|█████▏                                                   | 9/100 [01:04<08:35,  5.67s/it]                                                                                                9%|█████▏                                                   | 9/100 [01:04<08:35,  5.67s/it] 10%|█████▌                                                  | 10/100 [01:10<08:23,  5.59s/it]                                                                                               10%|█████▌                                                  | 10/100 [01:10<08:23,  5.59s/it] 11%|██████▏                                                 | 11/100 [01:15<08:15,  5.57s/it]                                                                                               11%|██████▏                                                 | 11/100 [01:15<08:15,  5.57s/it] 12%|██████▋                                                 | 12/100 [01:20<08:04,  5.51s/it]                                                                                               12%|██████▋                                                 | 12/100 [01:20<08:04,  5.51s/it] 13%|███████▎                                                | 13/100 [01:26<07:58,  5.50s/it]                                                                                               13%|███████▎                                                | 13/100 [01:26<07:58,  5.50s/it] 14%|███████▊                                                | 14/100 [01:32<07:57,  5.56s/it]                                                                                               14%|███████▊                                                | 14/100 [01:32<07:57,  5.56s/it] 15%|████████▍                                               | 15/100 [01:37<07:45,  5.48s/it]                                                                                               15%|████████▍                                               | 15/100 [01:37<07:45,  5.48s/it] 16%|████████▉                                               | 16/100 [01:42<07:34,  5.41s/it]                                                                                               16%|████████▉                                               | 16/100 [01:42<07:34,  5.41s/it] 17%|█████████▌                                              | 17/100 [01:48<07:32,  5.45s/it]                                                                                               17%|█████████▌                                              | 17/100 [01:48<07:32,  5.45s/it] 18%|██████████                                              | 18/100 [01:53<07:21,  5.38s/it]                                                                                               18%|██████████                                              | 18/100 [01:53<07:21,  5.38s/it] 19%|██████████▋                                             | 19/100 [01:59<07:25,  5.50s/it]                                                                                               19%|██████████▋                                             | 19/100 [01:59<07:25,  5.50s/it] 20%|███████████▏                                            | 20/100 [02:04<07:19,  5.49s/it]                                                                                               20%|███████████▏                                            | 20/100 [02:04<07:19,  5.49s/it] 21%|███████████▊                                            | 21/100 [02:09<07:07,  5.41s/it]                                                                                               21%|███████████▊                                            | 21/100 [02:09<07:07,  5.41s/it] 22%|████████████▎                                           | 22/100 [02:15<06:57,  5.35s/it]                                                                                               22%|████████████▎                                           | 22/100 [02:15<06:57,  5.35s/it] 23%|████████████▉                                           | 23/100 [02:20<07:00,  5.46s/it]                                                                                               23%|████████████▉                                           | 23/100 [02:20<07:00,  5.46s/it] 24%|█████████████▍                                          | 24/100 [02:26<06:59,  5.52s/it]                                                                                               24%|█████████████▍                                          | 24/100 [02:26<06:59,  5.52s/it] 25%|██████████████                                          | 25/100 [02:32<06:55,  5.54s/it]                                                                                               25%|██████████████                                          | 25/100 [02:32<06:55,  5.54s/it] 26%|██████████████▌                                         | 26/100 [02:37<06:48,  5.52s/it]                                                                                               26%|██████████████▌                                         | 26/100 [02:37<06:48,  5.52s/it] 27%|███████████████                                         | 27/100 [02:43<06:43,  5.52s/it]                                                                                               27%|███████████████                                         | 27/100 [02:43<06:43,  5.52s/it] 28%|███████████████▋                                        | 28/100 [02:48<06:37,  5.52s/it]                                                                                               28%|███████████████▋                                        | 28/100 [02:48<06:37,  5.52s/it] 29%|████████████████▏                                       | 29/100 [02:54<06:29,  5.49s/it]                                                                                               29%|████████████████▏                                       | 29/100 [02:54<06:29,  5.49s/it] 30%|████████████████▊                                       | 30/100 [02:59<06:24,  5.49s/it]                                                                                               30%|████████████████▊                                       | 30/100 [02:59<06:24,  5.49s/it] 31%|█████████████████▎                                      | 31/100 [03:04<06:12,  5.40s/it]                                                                                               31%|█████████████████▎                                      | 31/100 [03:04<06:12,  5.40s/it] 32%|█████████████████▉                                      | 32/100 [03:10<06:07,  5.41s/it]                                                                                               32%|█████████████████▉                                      | 32/100 [03:10<06:07,  5.41s/it] 33%|██████████████████▍                                     | 33/100 [03:16<06:13,  5.58s/it]                                                                                               33%|██████████████████▍                                     | 33/100 [03:16<06:13,  5.58s/it] 34%|███████████████████                                     | 34/100 [03:21<06:06,  5.55s/it]                                                                                               34%|███████████████████                                     | 34/100 [03:21<06:06,  5.55s/it] 35%|███████████████████▌                                    | 35/100 [03:27<05:59,  5.53s/it]                                                                                               35%|███████████████████▌                                    | 35/100 [03:27<05:59,  5.53s/it] 36%|████████████████████▏                                   | 36/100 [03:32<05:55,  5.56s/it]                                                                                               36%|████████████████████▏                                   | 36/100 [03:32<05:55,  5.56s/it] 37%|████████████████████▋                                   | 37/100 [03:38<05:51,  5.58s/it]                                                                                               37%|████████████████████▋                                   | 37/100 [03:38<05:51,  5.58s/it] 38%|█████████████████████▎                                  | 38/100 [03:44<05:48,  5.62s/it]                                                                                               38%|█████████████████████▎                                  | 38/100 [03:44<05:48,  5.62s/it] 39%|█████████████████████▊                                  | 39/100 [03:49<05:48,  5.72s/it]                                                                                               39%|█████████████████████▊                                  | 39/100 [03:50<05:48,  5.72s/it] 40%|██████████████████████▍                                 | 40/100 [03:55<05:33,  5.56s/it]                                                                                               40%|██████████████████████▍                                 | 40/100 [03:55<05:33,  5.56s/it] 41%|██████████████████████▉                                 | 41/100 [04:00<05:22,  5.46s/it]                                                                                               41%|██████████████████████▉                                 | 41/100 [04:00<05:22,  5.46s/it] 42%|███████████████████████▌                                | 42/100 [04:06<05:25,  5.61s/it]                                                                                               42%|███████████████████████▌                                | 42/100 [04:06<05:25,  5.61s/it] 43%|████████████████████████                                | 43/100 [04:11<05:15,  5.54s/it]                                                                                               43%|████████████████████████                                | 43/100 [04:11<05:15,  5.54s/it] 44%|████████████████████████▋                               | 44/100 [04:17<05:11,  5.57s/it]                                                                                               44%|████████████████████████▋                               | 44/100 [04:17<05:11,  5.57s/it] 45%|█████████████████████████▏                              | 45/100 [04:22<05:02,  5.51s/it]                                                                                               45%|█████████████████████████▏                              | 45/100 [04:22<05:02,  5.51s/it] 46%|█████████████████████████▊                              | 46/100 [04:27<04:52,  5.42s/it]                                                                                               46%|█████████████████████████▊                              | 46/100 [04:27<04:52,  5.42s/it] 47%|██████████████████████████▎                             | 47/100 [04:33<04:47,  5.42s/it]                                                                                               47%|██████████████████████████▎                             | 47/100 [04:33<04:47,  5.42s/it] 48%|██████████████████████████▉                             | 48/100 [04:38<04:38,  5.35s/it]                                                                                               48%|██████████████████████████▉                             | 48/100 [04:38<04:38,  5.35s/it] 49%|███████████████████████████▍                            | 49/100 [04:43<04:33,  5.37s/it]                                                                                               49%|███████████████████████████▍                            | 49/100 [04:44<04:33,  5.37s/it] 50%|████████████████████████████                            | 50/100 [04:49<04:25,  5.31s/it]                                                                                               50%|████████████████████████████                            | 50/100 [04:49<04:25,  5.31s/it] 51%|████████████████████████████▌                           | 51/100 [04:54<04:27,  5.46s/it]                                                                                               51%|████████████████████████████▌                           | 51/100 [04:54<04:27,  5.46s/it] 52%|█████████████████████████████                           | 52/100 [05:00<04:20,  5.42s/it]                                                                                               52%|█████████████████████████████                           | 52/100 [05:00<04:20,  5.42s/it] 53%|█████████████████████████████▋                          | 53/100 [05:05<04:13,  5.39s/it]                                                                                               53%|█████████████████████████████▋                          | 53/100 [05:05<04:13,  5.39s/it] 54%|██████████████████████████████▏                         | 54/100 [05:11<04:09,  5.42s/it]                                                                                               54%|██████████████████████████████▏                         | 54/100 [05:11<04:09,  5.42s/it] 55%|██████████████████████████████▊                         | 55/100 [05:16<04:09,  5.54s/it]                                                                                               55%|██████████████████████████████▊                         | 55/100 [05:16<04:09,  5.54s/it] 56%|███████████████████████████████▎                        | 56/100 [05:22<03:59,  5.44s/it]                                                                                               56%|███████████████████████████████▎                        | 56/100 [05:22<03:59,  5.44s/it] 57%|███████████████████████████████▉                        | 57/100 [05:27<03:55,  5.47s/it]                                                                                               57%|███████████████████████████████▉                        | 57/100 [05:27<03:55,  5.47s/it] 58%|████████████████████████████████▍                       | 58/100 [05:32<03:46,  5.39s/it]                                                                                               58%|████████████████████████████████▍                       | 58/100 [05:32<03:46,  5.39s/it] 59%|█████████████████████████████████                       | 59/100 [05:38<03:39,  5.35s/it]                                                                                               59%|█████████████████████████████████                       | 59/100 [05:38<03:39,  5.35s/it] 60%|█████████████████████████████████▌                      | 60/100 [05:43<03:34,  5.37s/it]                                                                                               60%|█████████████████████████████████▌                      | 60/100 [05:43<03:34,  5.37s/it] 61%|██████████████████████████████████▏                     | 61/100 [05:48<03:30,  5.39s/it]                                                                                               61%|██████████████████████████████████▏                     | 61/100 [05:49<03:30,  5.39s/it] 62%|██████████████████████████████████▋                     | 62/100 [05:54<03:24,  5.39s/it]                                                                                               62%|██████████████████████████████████▋                     | 62/100 [05:54<03:24,  5.39s/it] 63%|███████████████████████████████████▎                    | 63/100 [05:59<03:18,  5.37s/it]                                                                                               63%|███████████████████████████████████▎                    | 63/100 [05:59<03:18,  5.37s/it] 64%|███████████████████████████████████▊                    | 64/100 [06:05<03:14,  5.41s/it]                                                                                               64%|███████████████████████████████████▊                    | 64/100 [06:05<03:14,  5.41s/it] 65%|████████████████████████████████████▍                   | 65/100 [06:10<03:12,  5.50s/it]                                                                                               65%|████████████████████████████████████▍                   | 65/100 [06:10<03:12,  5.50s/it] 66%|████████████████████████████████████▉                   | 66/100 [06:16<03:11,  5.63s/it]                                                                                               66%|████████████████████████████████████▉                   | 66/100 [06:16<03:11,  5.63s/it] 67%|█████████████████████████████████████▌                  | 67/100 [06:22<03:04,  5.60s/it]                                                                                               67%|█████████████████████████████████████▌                  | 67/100 [06:22<03:04,  5.60s/it] 68%|██████████████████████████████████████                  | 68/100 [06:27<02:56,  5.52s/it]                                                                                               68%|██████████████████████████████████████                  | 68/100 [06:27<02:56,  5.52s/it] 69%|██████████████████████████████████████▋                 | 69/100 [06:33<02:52,  5.57s/it]                                                                                               69%|██████████████████████████████████████▋                 | 69/100 [06:33<02:52,  5.57s/it] 70%|███████████████████████████████████████▏                | 70/100 [06:39<02:48,  5.60s/it]                                                                                               70%|███████████████████████████████████████▏                | 70/100 [06:39<02:48,  5.60s/it] 71%|███████████████████████████████████████▊                | 71/100 [06:44<02:40,  5.53s/it]                                                                                               71%|███████████████████████████████████████▊                | 71/100 [06:44<02:40,  5.53s/it] 72%|████████████████████████████████████████▎               | 72/100 [06:50<02:39,  5.69s/it]                                                                                               72%|████████████████████████████████████████▎               | 72/100 [06:50<02:39,  5.69s/it] 73%|████████████████████████████████████████▉               | 73/100 [06:55<02:31,  5.60s/it]                                                                                               73%|████████████████████████████████████████▉               | 73/100 [06:55<02:31,  5.60s/it] 74%|█████████████████████████████████████████▍              | 74/100 [07:01<02:24,  5.58s/it]                                                                                               74%|█████████████████████████████████████████▍              | 74/100 [07:01<02:24,  5.58s/it] 75%|██████████████████████████████████████████              | 75/100 [07:07<02:19,  5.59s/it]                                                                                               75%|██████████████████████████████████████████              | 75/100 [07:07<02:19,  5.59s/it] 76%|██████████████████████████████████████████▌             | 76/100 [07:12<02:12,  5.53s/it]                                                                                               76%|██████████████████████████████████████████▌             | 76/100 [07:12<02:12,  5.53s/it] 77%|███████████████████████████████████████████             | 77/100 [07:17<02:05,  5.47s/it]                                                                                               77%|███████████████████████████████████████████             | 77/100 [07:17<02:05,  5.47s/it] 78%|███████████████████████████████████████████▋            | 78/100 [07:22<01:59,  5.41s/it]                                                                                               78%|███████████████████████████████████████████▋            | 78/100 [07:23<01:59,  5.41s/it] 79%|████████████████████████████████████████████▏           | 79/100 [07:28<01:53,  5.42s/it]                                                                                               79%|████████████████████████████████████████████▏           | 79/100 [07:28<01:53,  5.42s/it] 80%|████████████████████████████████████████████▊           | 80/100 [07:33<01:48,  5.41s/it]                                                                                               80%|████████████████████████████████████████████▊           | 80/100 [07:33<01:48,  5.41s/it] 81%|█████████████████████████████████████████████▎          | 81/100 [07:39<01:42,  5.37s/it]                                                                                               81%|█████████████████████████████████████████████▎          | 81/100 [07:39<01:42,  5.37s/it] 82%|█████████████████████████████████████████████▉          | 82/100 [07:44<01:36,  5.39s/it]                                                                                               82%|█████████████████████████████████████████████▉          | 82/100 [07:44<01:36,  5.39s/it] 83%|██████████████████████████████████████████████▍         | 83/100 [07:49<01:31,  5.38s/it]                                                                                               83%|██████████████████████████████████████████████▍         | 83/100 [07:49<01:31,  5.38s/it] 84%|███████████████████████████████████████████████         | 84/100 [07:55<01:26,  5.41s/it]                                                                                               84%|███████████████████████████████████████████████         | 84/100 [07:55<01:26,  5.41s/it] 85%|███████████████████████████████████████████████▌        | 85/100 [08:00<01:21,  5.43s/it]                                                                                               85%|███████████████████████████████████████████████▌        | 85/100 [08:00<01:21,  5.43s/it] 86%|████████████████████████████████████████████████▏       | 86/100 [08:06<01:16,  5.46s/it]                                                                                               86%|████████████████████████████████████████████████▏       | 86/100 [08:06<01:16,  5.46s/it] 87%|████████████████████████████████████████████████▋       | 87/100 [08:11<01:11,  5.49s/it]                                                                                               87%|████████████████████████████████████████████████▋       | 87/100 [08:11<01:11,  5.49s/it] 88%|█████████████████████████████████████████████████▎      | 88/100 [08:17<01:05,  5.46s/it]                                                                                               88%|█████████████████████████████████████████████████▎      | 88/100 [08:17<01:05,  5.46s/it] 89%|█████████████████████████████████████████████████▊      | 89/100 [08:22<01:00,  5.50s/it]                                                                                               89%|█████████████████████████████████████████████████▊      | 89/100 [08:22<01:00,  5.50s/it] 90%|██████████████████████████████████████████████████▍     | 90/100 [08:28<00:56,  5.62s/it]                                                                                               90%|██████████████████████████████████████████████████▍     | 90/100 [08:28<00:56,  5.62s/it] 91%|██████████████████████████████████████████████████▉     | 91/100 [08:34<00:50,  5.59s/it]                                                                                               91%|██████████████████████████████████████████████████▉     | 91/100 [08:34<00:50,  5.59s/it] 92%|███████████████████████████████████████████████████▌    | 92/100 [08:39<00:44,  5.59s/it]                                                                                               92%|███████████████████████████████████████████████████▌    | 92/100 [08:39<00:44,  5.59s/it] 93%|████████████████████████████████████████████████████    | 93/100 [08:45<00:38,  5.54s/it]                                                                                               93%|████████████████████████████████████████████████████    | 93/100 [08:45<00:38,  5.54s/it] 94%|████████████████████████████████████████████████████▋   | 94/100 [08:50<00:33,  5.53s/it]                                                                                               94%|████████████████████████████████████████████████████▋   | 94/100 [08:50<00:33,  5.53s/it] 95%|█████████████████████████████████████████████████████▏  | 95/100 [08:56<00:27,  5.49s/it]                                                                                               95%|█████████████████████████████████████████████████████▏  | 95/100 [08:56<00:27,  5.49s/it] 96%|█████████████████████████████████████████████████████▊  | 96/100 [09:01<00:21,  5.46s/it]                                                                                               96%|█████████████████████████████████████████████████████▊  | 96/100 [09:01<00:21,  5.46s/it] 97%|██████████████████████████████████████████████████████▎ | 97/100 [09:06<00:16,  5.41s/it]                                                                                               97%|██████████████████████████████████████████████████████▎ | 97/100 [09:07<00:16,  5.41s/it] 98%|██████████████████████████████████████████████████████▉ | 98/100 [09:12<00:10,  5.45s/it]                                                                                               98%|██████████████████████████████████████████████████████▉ | 98/100 [09:12<00:10,  5.45s/it] 99%|███████████████████████████████████████████████████████▍| 99/100 [09:17<00:05,  5.43s/it]                                                                                               99%|███████████████████████████████████████████████████████▍| 99/100 [09:17<00:05,  5.43s/it]100%|███████████████████████████████████████████████████████| 100/100 [09:22<00:00,  5.34s/it]                                                                                              100%|███████████████████████████████████████████████████████| 100/100 [09:23<00:00,  5.34s/it]                                                                                              100%|███████████████████████████████████████████████████████| 100/100 [09:23<00:00,  5.34s/it]100%|███████████████████████████████████████████████████████| 100/100 [09:23<00:00,  5.63s/it]
[rank0]:[W220 01:02:14.366938304 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
