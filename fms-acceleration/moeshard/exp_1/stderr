/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  6.46it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.41it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.24it/s]
WARNING:sft_trainer.py:PAD token set to default, to make it different from eos token
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  7.87it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.94it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  6.34it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  9.03it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.98it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.79it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.27it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.11it/s]
WARNING:sft_trainer.py:PAD token set to default, to make it different from eos token
WARNING:sft_trainer.py:PAD token set to default, to make it different from eos token
WARNING:sft_trainer.py:PAD token set to default, to make it different from eos token
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
/opt/conda/envs/tp/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
/opt/conda/envs/tp/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
/opt/conda/envs/tp/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
  0%|          | 0/100 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  1%|          | 1/100 [00:02<04:40,  2.84s/it]                                                 1%|          | 1/100 [00:02<04:40,  2.84s/it]  2%|▏         | 2/100 [00:04<03:24,  2.08s/it]                                                 2%|▏         | 2/100 [00:04<03:24,  2.08s/it]  3%|▎         | 3/100 [00:05<02:59,  1.85s/it]                                                 3%|▎         | 3/100 [00:05<02:59,  1.85s/it]  4%|▍         | 4/100 [00:07<02:45,  1.73s/it]                                                 4%|▍         | 4/100 [00:07<02:45,  1.73s/it]  5%|▌         | 5/100 [00:09<02:36,  1.65s/it]                                                 5%|▌         | 5/100 [00:09<02:36,  1.65s/it]  6%|▌         | 6/100 [00:10<02:32,  1.62s/it]                                                 6%|▌         | 6/100 [00:10<02:32,  1.62s/it]  7%|▋         | 7/100 [00:12<02:28,  1.59s/it]                                                 7%|▋         | 7/100 [00:12<02:28,  1.59s/it]  8%|▊         | 8/100 [00:13<02:26,  1.60s/it]                                                 8%|▊         | 8/100 [00:13<02:26,  1.60s/it]  9%|▉         | 9/100 [00:15<02:24,  1.59s/it]                                                 9%|▉         | 9/100 [00:15<02:24,  1.59s/it] 10%|█         | 10/100 [00:16<02:21,  1.57s/it]                                                 10%|█         | 10/100 [00:16<02:21,  1.57s/it] 11%|█         | 11/100 [00:18<02:20,  1.58s/it]                                                 11%|█         | 11/100 [00:18<02:20,  1.58s/it] 12%|█▏        | 12/100 [00:20<02:20,  1.59s/it]                                                 12%|█▏        | 12/100 [00:20<02:20,  1.59s/it] 13%|█▎        | 13/100 [00:21<02:17,  1.58s/it]                                                 13%|█▎        | 13/100 [00:21<02:17,  1.58s/it] 14%|█▍        | 14/100 [00:23<02:15,  1.57s/it]                                                 14%|█▍        | 14/100 [00:23<02:15,  1.57s/it] 15%|█▌        | 15/100 [00:24<02:15,  1.59s/it]                                                 15%|█▌        | 15/100 [00:24<02:15,  1.59s/it] 16%|█▌        | 16/100 [00:26<02:13,  1.58s/it]                                                 16%|█▌        | 16/100 [00:26<02:13,  1.58s/it] 17%|█▋        | 17/100 [00:27<02:10,  1.57s/it]                                                 17%|█▋        | 17/100 [00:27<02:10,  1.57s/it] 18%|█▊        | 18/100 [00:29<02:08,  1.56s/it]                                                 18%|█▊        | 18/100 [00:29<02:08,  1.56s/it] 19%|█▉        | 19/100 [00:30<02:06,  1.56s/it]                                                 19%|█▉        | 19/100 [00:31<02:06,  1.56s/it] 20%|██        | 20/100 [00:32<02:06,  1.58s/it]                                                 20%|██        | 20/100 [00:32<02:06,  1.58s/it] 21%|██        | 21/100 [00:34<02:03,  1.56s/it]                                                 21%|██        | 21/100 [00:34<02:03,  1.56s/it] 22%|██▏       | 22/100 [00:35<02:02,  1.57s/it]                                                 22%|██▏       | 22/100 [00:35<02:02,  1.57s/it] 23%|██▎       | 23/100 [00:37<02:01,  1.58s/it]                                                 23%|██▎       | 23/100 [00:37<02:01,  1.58s/it] 24%|██▍       | 24/100 [00:38<01:59,  1.57s/it]                                                 24%|██▍       | 24/100 [00:38<01:59,  1.57s/it] 25%|██▌       | 25/100 [00:40<01:57,  1.56s/it]                                                 25%|██▌       | 25/100 [00:40<01:57,  1.56s/it] 26%|██▌       | 26/100 [00:41<01:54,  1.55s/it]                                                 26%|██▌       | 26/100 [00:41<01:54,  1.55s/it] 27%|██▋       | 27/100 [00:43<01:53,  1.56s/it]                                                 27%|██▋       | 27/100 [00:43<01:53,  1.56s/it] 28%|██▊       | 28/100 [00:45<01:51,  1.55s/it]                                                 28%|██▊       | 28/100 [00:45<01:51,  1.55s/it] 29%|██▉       | 29/100 [00:46<01:50,  1.56s/it]                                                 29%|██▉       | 29/100 [00:46<01:50,  1.56s/it] 30%|███       | 30/100 [00:48<01:48,  1.56s/it]                                                 30%|███       | 30/100 [00:48<01:48,  1.56s/it] 31%|███       | 31/100 [00:49<01:47,  1.56s/it]                                                 31%|███       | 31/100 [00:49<01:47,  1.56s/it] 32%|███▏      | 32/100 [00:51<01:46,  1.57s/it]                                                 32%|███▏      | 32/100 [00:51<01:46,  1.57s/it] 33%|███▎      | 33/100 [00:52<01:45,  1.57s/it]                                                 33%|███▎      | 33/100 [00:52<01:45,  1.57s/it] 34%|███▍      | 34/100 [00:54<01:43,  1.56s/it]                                                 34%|███▍      | 34/100 [00:54<01:43,  1.56s/it] 35%|███▌      | 35/100 [00:56<01:42,  1.57s/it]                                                 35%|███▌      | 35/100 [00:56<01:42,  1.57s/it] 36%|███▌      | 36/100 [00:57<01:41,  1.58s/it]                                                 36%|███▌      | 36/100 [00:57<01:41,  1.58s/it] 37%|███▋      | 37/100 [00:59<01:40,  1.59s/it]                                                 37%|███▋      | 37/100 [00:59<01:40,  1.59s/it] 38%|███▊      | 38/100 [01:00<01:36,  1.55s/it]                                                 38%|███▊      | 38/100 [01:00<01:36,  1.55s/it] 39%|███▉      | 39/100 [01:02<01:33,  1.52s/it]                                                 39%|███▉      | 39/100 [01:02<01:33,  1.52s/it] 40%|████      | 40/100 [01:03<01:32,  1.53s/it]                                                 40%|████      | 40/100 [01:03<01:32,  1.53s/it] 41%|████      | 41/100 [01:05<01:30,  1.54s/it]                                                 41%|████      | 41/100 [01:05<01:30,  1.54s/it] 42%|████▏     | 42/100 [01:06<01:29,  1.55s/it]                                                 42%|████▏     | 42/100 [01:06<01:29,  1.55s/it] 43%|████▎     | 43/100 [01:08<01:29,  1.57s/it]                                                 43%|████▎     | 43/100 [01:08<01:29,  1.57s/it] 44%|████▍     | 44/100 [01:09<01:25,  1.53s/it]                                                 44%|████▍     | 44/100 [01:09<01:25,  1.53s/it] 45%|████▌     | 45/100 [01:11<01:23,  1.52s/it]                                                 45%|████▌     | 45/100 [01:11<01:23,  1.52s/it] 46%|████▌     | 46/100 [01:13<01:23,  1.54s/it]                                                 46%|████▌     | 46/100 [01:13<01:23,  1.54s/it] 47%|████▋     | 47/100 [01:14<01:19,  1.50s/it]                                                 47%|████▋     | 47/100 [01:14<01:19,  1.50s/it] 48%|████▊     | 48/100 [01:15<01:17,  1.50s/it]                                                 48%|████▊     | 48/100 [01:15<01:17,  1.50s/it] 49%|████▉     | 49/100 [01:17<01:17,  1.51s/it]                                                 49%|████▉     | 49/100 [01:17<01:17,  1.51s/it] 50%|█████     | 50/100 [01:19<01:16,  1.52s/it]                                                 50%|█████     | 50/100 [01:19<01:16,  1.52s/it] 51%|█████     | 51/100 [01:20<01:14,  1.52s/it]                                                 51%|█████     | 51/100 [01:20<01:14,  1.52s/it] 52%|█████▏    | 52/100 [01:22<01:12,  1.51s/it]                                                 52%|█████▏    | 52/100 [01:22<01:12,  1.51s/it] 53%|█████▎    | 53/100 [01:23<01:13,  1.56s/it]                                                 53%|█████▎    | 53/100 [01:23<01:13,  1.56s/it] 54%|█████▍    | 54/100 [01:25<01:10,  1.54s/it]                                                 54%|█████▍    | 54/100 [01:25<01:10,  1.54s/it] 55%|█████▌    | 55/100 [01:26<01:09,  1.54s/it]                                                 55%|█████▌    | 55/100 [01:26<01:09,  1.54s/it] 56%|█████▌    | 56/100 [01:28<01:07,  1.54s/it]                                                 56%|█████▌    | 56/100 [01:28<01:07,  1.54s/it] 57%|█████▋    | 57/100 [01:29<01:05,  1.52s/it]                                                 57%|█████▋    | 57/100 [01:29<01:05,  1.52s/it] 58%|█████▊    | 58/100 [01:31<01:03,  1.51s/it]                                                 58%|█████▊    | 58/100 [01:31<01:03,  1.51s/it] 59%|█████▉    | 59/100 [01:32<01:02,  1.53s/it]                                                 59%|█████▉    | 59/100 [01:32<01:02,  1.53s/it] 60%|██████    | 60/100 [01:34<01:00,  1.52s/it]                                                 60%|██████    | 60/100 [01:34<01:00,  1.52s/it] 61%|██████    | 61/100 [01:35<00:58,  1.50s/it]                                                 61%|██████    | 61/100 [01:35<00:58,  1.50s/it] 62%|██████▏   | 62/100 [01:37<00:57,  1.53s/it]                                                 62%|██████▏   | 62/100 [01:37<00:57,  1.53s/it] 63%|██████▎   | 63/100 [01:38<00:56,  1.53s/it]                                                 63%|██████▎   | 63/100 [01:38<00:56,  1.53s/it] 64%|██████▍   | 64/100 [01:40<00:54,  1.51s/it]                                                 64%|██████▍   | 64/100 [01:40<00:54,  1.51s/it] 65%|██████▌   | 65/100 [01:41<00:53,  1.52s/it]                                                 65%|██████▌   | 65/100 [01:41<00:53,  1.52s/it] 66%|██████▌   | 66/100 [01:43<00:50,  1.49s/it]                                                 66%|██████▌   | 66/100 [01:43<00:50,  1.49s/it] 67%|██████▋   | 67/100 [01:44<00:49,  1.51s/it]                                                 67%|██████▋   | 67/100 [01:44<00:49,  1.51s/it] 68%|██████▊   | 68/100 [01:46<00:49,  1.53s/it]                                                 68%|██████▊   | 68/100 [01:46<00:49,  1.53s/it] 69%|██████▉   | 69/100 [01:48<00:47,  1.54s/it]                                                 69%|██████▉   | 69/100 [01:48<00:47,  1.54s/it] 70%|███████   | 70/100 [01:49<00:46,  1.53s/it]                                                 70%|███████   | 70/100 [01:49<00:46,  1.53s/it] 71%|███████   | 71/100 [01:50<00:43,  1.51s/it]                                                 71%|███████   | 71/100 [01:51<00:43,  1.51s/it] 72%|███████▏  | 72/100 [01:52<00:41,  1.49s/it]                                                 72%|███████▏  | 72/100 [01:52<00:41,  1.49s/it] 73%|███████▎  | 73/100 [01:54<00:41,  1.52s/it]                                                 73%|███████▎  | 73/100 [01:54<00:41,  1.52s/it] 74%|███████▍  | 74/100 [01:55<00:39,  1.54s/it]                                                 74%|███████▍  | 74/100 [01:55<00:39,  1.54s/it] 75%|███████▌  | 75/100 [01:57<00:37,  1.51s/it]                                                 75%|███████▌  | 75/100 [01:57<00:37,  1.51s/it] 76%|███████▌  | 76/100 [01:58<00:36,  1.54s/it]                                                 76%|███████▌  | 76/100 [01:58<00:36,  1.54s/it] 77%|███████▋  | 77/100 [02:00<00:34,  1.52s/it]                                                 77%|███████▋  | 77/100 [02:00<00:34,  1.52s/it] 78%|███████▊  | 78/100 [02:01<00:33,  1.52s/it]                                                 78%|███████▊  | 78/100 [02:01<00:33,  1.52s/it] 79%|███████▉  | 79/100 [02:03<00:31,  1.52s/it]                                                 79%|███████▉  | 79/100 [02:03<00:31,  1.52s/it] 80%|████████  | 80/100 [02:04<00:30,  1.53s/it]                                                 80%|████████  | 80/100 [02:04<00:30,  1.53s/it] 81%|████████  | 81/100 [02:06<00:28,  1.51s/it]                                                 81%|████████  | 81/100 [02:06<00:28,  1.51s/it] 82%|████████▏ | 82/100 [02:07<00:27,  1.50s/it]                                                 82%|████████▏ | 82/100 [02:07<00:27,  1.50s/it] 83%|████████▎ | 83/100 [02:09<00:25,  1.50s/it]                                                 83%|████████▎ | 83/100 [02:09<00:25,  1.50s/it] 84%|████████▍ | 84/100 [02:10<00:24,  1.50s/it]                                                 84%|████████▍ | 84/100 [02:10<00:24,  1.50s/it] 85%|████████▌ | 85/100 [02:12<00:22,  1.51s/it]                                                 85%|████████▌ | 85/100 [02:12<00:22,  1.51s/it] 86%|████████▌ | 86/100 [02:13<00:21,  1.51s/it]                                                 86%|████████▌ | 86/100 [02:13<00:21,  1.51s/it] 87%|████████▋ | 87/100 [02:15<00:19,  1.50s/it]                                                 87%|████████▋ | 87/100 [02:15<00:19,  1.50s/it] 88%|████████▊ | 88/100 [02:16<00:17,  1.48s/it]                                                 88%|████████▊ | 88/100 [02:16<00:17,  1.48s/it] 89%|████████▉ | 89/100 [02:18<00:16,  1.51s/it]                                                 89%|████████▉ | 89/100 [02:18<00:16,  1.51s/it] 90%|█████████ | 90/100 [02:19<00:15,  1.53s/it]                                                 90%|█████████ | 90/100 [02:19<00:15,  1.53s/it] 91%|█████████ | 91/100 [02:21<00:13,  1.54s/it]                                                 91%|█████████ | 91/100 [02:21<00:13,  1.54s/it] 92%|█████████▏| 92/100 [02:22<00:12,  1.54s/it]                                                 92%|█████████▏| 92/100 [02:22<00:12,  1.54s/it] 93%|█████████▎| 93/100 [02:24<00:10,  1.52s/it]                                                 93%|█████████▎| 93/100 [02:24<00:10,  1.52s/it] 94%|█████████▍| 94/100 [02:25<00:09,  1.52s/it]                                                 94%|█████████▍| 94/100 [02:25<00:09,  1.52s/it] 95%|█████████▌| 95/100 [02:27<00:07,  1.55s/it]                                                 95%|█████████▌| 95/100 [02:27<00:07,  1.55s/it] 96%|█████████▌| 96/100 [02:29<00:06,  1.61s/it]                                                 96%|█████████▌| 96/100 [02:29<00:06,  1.61s/it] 97%|█████████▋| 97/100 [02:30<00:04,  1.59s/it]                                                 97%|█████████▋| 97/100 [02:30<00:04,  1.59s/it] 98%|█████████▊| 98/100 [02:32<00:03,  1.58s/it]                                                 98%|█████████▊| 98/100 [02:32<00:03,  1.58s/it] 99%|█████████▉| 99/100 [02:33<00:01,  1.58s/it]                                                 99%|█████████▉| 99/100 [02:33<00:01,  1.58s/it]100%|██████████| 100/100 [02:35<00:00,  1.54s/it]                                                 100%|██████████| 100/100 [02:35<00:00,  1.54s/it]                                                 100%|██████████| 100/100 [02:35<00:00,  1.54s/it]100%|██████████| 100/100 [02:35<00:00,  1.56s/it]
[rank0]:[W218 19:02:52.095584457 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
