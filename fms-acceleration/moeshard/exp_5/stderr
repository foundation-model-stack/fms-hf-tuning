/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  7.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.62it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.51it/s]
WARNING:sft_trainer.py:PAD token set to default, to make it different from eos token
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  6.02it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.95it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.92it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.74it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.61it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.80it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.65it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.62it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.50it/s]
WARNING:sft_trainer.py:PAD token set to default, to make it different from eos token
WARNING:sft_trainer.py:PAD token set to default, to make it different from eos token
WARNING:sft_trainer.py:PAD token set to default, to make it different from eos token
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
You are using a model of type granitemoe to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
You are using a model of type granitemoe to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
You are using a model of type granitemoe to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
Converting ScatterMoE layers:   0%|          | 0/32 [00:00<?, ?it/s]You are using a model of type granitemoe to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
Converting ScatterMoE layers:   3%|▎         | 1/32 [00:01<00:47,  1.52s/it]Converting ScatterMoE layers:   9%|▉         | 3/32 [00:01<00:12,  2.26it/s]Converting ScatterMoE layers:  16%|█▌        | 5/32 [00:01<00:06,  4.03it/s]Converting ScatterMoE layers:  22%|██▏       | 7/32 [00:01<00:04,  5.88it/s]Converting ScatterMoE layers:  28%|██▊       | 9/32 [00:02<00:02,  7.98it/s]Converting ScatterMoE layers:  34%|███▍      | 11/32 [00:02<00:02,  9.45it/s]Converting ScatterMoE layers:  41%|████      | 13/32 [00:02<00:01, 10.83it/s]Converting ScatterMoE layers:  47%|████▋     | 15/32 [00:02<00:01, 12.15it/s]Converting ScatterMoE layers:  53%|█████▎    | 17/32 [00:02<00:01, 12.97it/s]Converting ScatterMoE layers:  59%|█████▉    | 19/32 [00:02<00:00, 14.06it/s]Converting ScatterMoE layers:  66%|██████▌   | 21/32 [00:02<00:00, 15.10it/s]Converting ScatterMoE layers:  72%|███████▏  | 23/32 [00:02<00:00, 15.63it/s]Converting ScatterMoE layers:  78%|███████▊  | 25/32 [00:02<00:00, 16.13it/s]Converting ScatterMoE layers:  84%|████████▍ | 27/32 [00:03<00:00, 16.45it/s]Converting ScatterMoE layers:  91%|█████████ | 29/32 [00:03<00:00, 15.88it/s]Converting ScatterMoE layers:  97%|█████████▋| 31/32 [00:03<00:00, 15.97it/s]Converting ScatterMoE layers: 100%|██████████| 32/32 [00:03<00:00,  9.32it/s]
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
/opt/conda/envs/tp/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
/opt/conda/envs/tp/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
/opt/conda/envs/tp/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/opt/conda/envs/tp/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
  0%|          | 0/100 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  1%|          | 1/100 [00:05<09:13,  5.59s/it]                                                 1%|          | 1/100 [00:05<09:13,  5.59s/it]  2%|▏         | 2/100 [00:06<04:40,  2.86s/it]                                                 2%|▏         | 2/100 [00:06<04:40,  2.86s/it]  3%|▎         | 3/100 [00:07<03:10,  1.96s/it]                                                 3%|▎         | 3/100 [00:07<03:10,  1.96s/it]  4%|▍         | 4/100 [00:08<02:28,  1.54s/it]                                                 4%|▍         | 4/100 [00:08<02:28,  1.54s/it]  5%|▌         | 5/100 [00:09<02:04,  1.31s/it]                                                 5%|▌         | 5/100 [00:09<02:04,  1.31s/it]  6%|▌         | 6/100 [00:10<01:50,  1.17s/it]                                                 6%|▌         | 6/100 [00:10<01:50,  1.17s/it]  7%|▋         | 7/100 [00:11<01:40,  1.08s/it]                                                 7%|▋         | 7/100 [00:11<01:40,  1.08s/it]  8%|▊         | 8/100 [00:11<01:33,  1.02s/it]                                                 8%|▊         | 8/100 [00:11<01:33,  1.02s/it]  9%|▉         | 9/100 [00:12<01:29,  1.01it/s]                                                 9%|▉         | 9/100 [00:12<01:29,  1.01it/s] 10%|█         | 10/100 [00:13<01:27,  1.03it/s]                                                 10%|█         | 10/100 [00:13<01:27,  1.03it/s] 11%|█         | 11/100 [00:14<01:24,  1.05it/s]                                                 11%|█         | 11/100 [00:14<01:24,  1.05it/s] 12%|█▏        | 12/100 [00:15<01:22,  1.06it/s]                                                 12%|█▏        | 12/100 [00:15<01:22,  1.06it/s] 13%|█▎        | 13/100 [00:16<01:20,  1.08it/s]                                                 13%|█▎        | 13/100 [00:16<01:20,  1.08it/s] 14%|█▍        | 14/100 [00:17<01:18,  1.09it/s]                                                 14%|█▍        | 14/100 [00:17<01:18,  1.09it/s] 15%|█▌        | 15/100 [00:18<01:17,  1.09it/s]                                                 15%|█▌        | 15/100 [00:18<01:17,  1.09it/s] 16%|█▌        | 16/100 [00:19<01:17,  1.09it/s]                                                 16%|█▌        | 16/100 [00:19<01:17,  1.09it/s] 17%|█▋        | 17/100 [00:20<01:15,  1.10it/s]                                                 17%|█▋        | 17/100 [00:20<01:15,  1.10it/s] 18%|█▊        | 18/100 [00:20<01:13,  1.11it/s]                                                 18%|█▊        | 18/100 [00:21<01:13,  1.11it/s] 19%|█▉        | 19/100 [00:21<01:12,  1.11it/s]                                                 19%|█▉        | 19/100 [00:21<01:12,  1.11it/s] 20%|██        | 20/100 [00:22<01:12,  1.10it/s]                                                 20%|██        | 20/100 [00:22<01:12,  1.10it/s] 21%|██        | 21/100 [00:23<01:11,  1.11it/s]                                                 21%|██        | 21/100 [00:23<01:11,  1.11it/s] 22%|██▏       | 22/100 [00:24<01:10,  1.11it/s]                                                 22%|██▏       | 22/100 [00:24<01:10,  1.11it/s] 23%|██▎       | 23/100 [00:25<01:09,  1.11it/s]                                                 23%|██▎       | 23/100 [00:25<01:09,  1.11it/s] 24%|██▍       | 24/100 [00:26<01:08,  1.11it/s]                                                 24%|██▍       | 24/100 [00:26<01:08,  1.11it/s] 25%|██▌       | 25/100 [00:27<01:07,  1.12it/s]                                                 25%|██▌       | 25/100 [00:27<01:07,  1.12it/s] 26%|██▌       | 26/100 [00:28<01:05,  1.13it/s]                                                 26%|██▌       | 26/100 [00:28<01:05,  1.13it/s] 27%|██▋       | 27/100 [00:29<01:05,  1.12it/s]                                                 27%|██▋       | 27/100 [00:29<01:05,  1.12it/s] 28%|██▊       | 28/100 [00:29<01:04,  1.12it/s]                                                 28%|██▊       | 28/100 [00:29<01:04,  1.12it/s] 29%|██▉       | 29/100 [00:30<01:03,  1.12it/s]                                                 29%|██▉       | 29/100 [00:30<01:03,  1.12it/s] 30%|███       | 30/100 [00:31<01:02,  1.13it/s]                                                 30%|███       | 30/100 [00:31<01:02,  1.13it/s] 31%|███       | 31/100 [00:32<01:01,  1.12it/s]                                                 31%|███       | 31/100 [00:32<01:01,  1.12it/s] 32%|███▏      | 32/100 [00:33<01:01,  1.11it/s]                                                 32%|███▏      | 32/100 [00:33<01:01,  1.11it/s] 33%|███▎      | 33/100 [00:34<00:59,  1.12it/s]                                                 33%|███▎      | 33/100 [00:34<00:59,  1.12it/s] 34%|███▍      | 34/100 [00:35<00:59,  1.10it/s]                                                 34%|███▍      | 34/100 [00:35<00:59,  1.10it/s] 35%|███▌      | 35/100 [00:36<00:58,  1.10it/s]                                                 35%|███▌      | 35/100 [00:36<00:58,  1.10it/s] 36%|███▌      | 36/100 [00:37<00:58,  1.10it/s]                                                 36%|███▌      | 36/100 [00:37<00:58,  1.10it/s] 37%|███▋      | 37/100 [00:38<00:57,  1.10it/s]                                                 37%|███▋      | 37/100 [00:38<00:57,  1.10it/s] 38%|███▊      | 38/100 [00:38<00:55,  1.11it/s]                                                 38%|███▊      | 38/100 [00:38<00:55,  1.11it/s] 39%|███▉      | 39/100 [00:39<00:55,  1.11it/s]                                                 39%|███▉      | 39/100 [00:39<00:55,  1.11it/s] 40%|████      | 40/100 [00:40<00:54,  1.11it/s]                                                 40%|████      | 40/100 [00:40<00:54,  1.11it/s] 41%|████      | 41/100 [00:41<00:52,  1.12it/s]                                                 41%|████      | 41/100 [00:41<00:52,  1.12it/s] 42%|████▏     | 42/100 [00:42<00:52,  1.11it/s]                                                 42%|████▏     | 42/100 [00:42<00:52,  1.11it/s] 43%|████▎     | 43/100 [00:43<00:51,  1.11it/s]                                                 43%|████▎     | 43/100 [00:43<00:51,  1.11it/s] 44%|████▍     | 44/100 [00:44<00:50,  1.11it/s]                                                 44%|████▍     | 44/100 [00:44<00:50,  1.11it/s] 45%|████▌     | 45/100 [00:45<00:49,  1.10it/s]                                                 45%|████▌     | 45/100 [00:45<00:49,  1.10it/s] 46%|████▌     | 46/100 [00:46<00:49,  1.10it/s]                                                 46%|████▌     | 46/100 [00:46<00:49,  1.10it/s] 47%|████▋     | 47/100 [00:47<00:47,  1.12it/s]                                                 47%|████▋     | 47/100 [00:47<00:47,  1.12it/s] 48%|████▊     | 48/100 [00:47<00:46,  1.11it/s]                                                 48%|████▊     | 48/100 [00:48<00:46,  1.11it/s] 49%|████▉     | 49/100 [00:48<00:46,  1.09it/s]                                                 49%|████▉     | 49/100 [00:48<00:46,  1.09it/s] 50%|█████     | 50/100 [00:49<00:45,  1.10it/s]                                                 50%|█████     | 50/100 [00:49<00:45,  1.10it/s] 51%|█████     | 51/100 [00:50<00:44,  1.10it/s]                                                 51%|█████     | 51/100 [00:50<00:44,  1.10it/s] 52%|█████▏    | 52/100 [00:51<00:43,  1.09it/s]                                                 52%|█████▏    | 52/100 [00:51<00:43,  1.09it/s] 53%|█████▎    | 53/100 [00:52<00:43,  1.09it/s]                                                 53%|█████▎    | 53/100 [00:52<00:43,  1.09it/s] 54%|█████▍    | 54/100 [00:53<00:41,  1.11it/s]                                                 54%|█████▍    | 54/100 [00:53<00:41,  1.11it/s] 55%|█████▌    | 55/100 [00:54<00:40,  1.11it/s]                                                 55%|█████▌    | 55/100 [00:54<00:40,  1.11it/s] 56%|█████▌    | 56/100 [00:55<00:39,  1.11it/s]                                                 56%|█████▌    | 56/100 [00:55<00:39,  1.11it/s] 57%|█████▋    | 57/100 [00:56<00:38,  1.11it/s]                                                 57%|█████▋    | 57/100 [00:56<00:38,  1.11it/s] 58%|█████▊    | 58/100 [00:57<00:37,  1.11it/s]                                                 58%|█████▊    | 58/100 [00:57<00:37,  1.11it/s] 59%|█████▉    | 59/100 [00:57<00:37,  1.10it/s]                                                 59%|█████▉    | 59/100 [00:58<00:37,  1.10it/s] 60%|██████    | 60/100 [00:59<00:37,  1.05it/s]                                                 60%|██████    | 60/100 [00:59<00:37,  1.05it/s] 61%|██████    | 61/100 [01:00<00:38,  1.02it/s]                                                 61%|██████    | 61/100 [01:00<00:38,  1.02it/s] 62%|██████▏   | 62/100 [01:01<00:38,  1.02s/it]                                                 62%|██████▏   | 62/100 [01:01<00:38,  1.02s/it] 63%|██████▎   | 63/100 [01:02<00:36,  1.02it/s]                                                 63%|██████▎   | 63/100 [01:02<00:36,  1.02it/s] 64%|██████▍   | 64/100 [01:02<00:34,  1.06it/s]                                                 64%|██████▍   | 64/100 [01:02<00:34,  1.06it/s] 65%|██████▌   | 65/100 [01:03<00:32,  1.07it/s]                                                 65%|██████▌   | 65/100 [01:03<00:32,  1.07it/s] 66%|██████▌   | 66/100 [01:04<00:31,  1.08it/s]                                                 66%|██████▌   | 66/100 [01:04<00:31,  1.08it/s] 67%|██████▋   | 67/100 [01:05<00:30,  1.10it/s]                                                 67%|██████▋   | 67/100 [01:05<00:30,  1.10it/s] 68%|██████▊   | 68/100 [01:06<00:29,  1.10it/s]                                                 68%|██████▊   | 68/100 [01:06<00:29,  1.10it/s] 69%|██████▉   | 69/100 [01:07<00:28,  1.10it/s]                                                 69%|██████▉   | 69/100 [01:07<00:28,  1.10it/s] 70%|███████   | 70/100 [01:08<00:27,  1.11it/s]                                                 70%|███████   | 70/100 [01:08<00:27,  1.11it/s] 71%|███████   | 71/100 [01:09<00:26,  1.11it/s]                                                 71%|███████   | 71/100 [01:09<00:26,  1.11it/s] 72%|███████▏  | 72/100 [01:10<00:25,  1.11it/s]                                                 72%|███████▏  | 72/100 [01:10<00:25,  1.11it/s] 73%|███████▎  | 73/100 [01:11<00:24,  1.11it/s]                                                 73%|███████▎  | 73/100 [01:11<00:24,  1.11it/s] 74%|███████▍  | 74/100 [01:11<00:23,  1.11it/s]                                                 74%|███████▍  | 74/100 [01:11<00:23,  1.11it/s] 75%|███████▌  | 75/100 [01:12<00:22,  1.12it/s]                                                 75%|███████▌  | 75/100 [01:12<00:22,  1.12it/s] 76%|███████▌  | 76/100 [01:13<00:21,  1.12it/s]                                                 76%|███████▌  | 76/100 [01:13<00:21,  1.12it/s] 77%|███████▋  | 77/100 [01:14<00:20,  1.12it/s]                                                 77%|███████▋  | 77/100 [01:14<00:20,  1.12it/s] 78%|███████▊  | 78/100 [01:15<00:19,  1.11it/s]                                                 78%|███████▊  | 78/100 [01:15<00:19,  1.11it/s] 79%|███████▉  | 79/100 [01:16<00:18,  1.12it/s]                                                 79%|███████▉  | 79/100 [01:16<00:18,  1.12it/s] 80%|████████  | 80/100 [01:17<00:17,  1.11it/s]                                                 80%|████████  | 80/100 [01:17<00:17,  1.11it/s] 81%|████████  | 81/100 [01:18<00:17,  1.11it/s]                                                 81%|████████  | 81/100 [01:18<00:17,  1.11it/s] 82%|████████▏ | 82/100 [01:19<00:16,  1.11it/s]                                                 82%|████████▏ | 82/100 [01:19<00:16,  1.11it/s] 83%|████████▎ | 83/100 [01:20<00:15,  1.11it/s]                                                 83%|████████▎ | 83/100 [01:20<00:15,  1.11it/s] 84%|████████▍ | 84/100 [01:20<00:14,  1.12it/s]                                                 84%|████████▍ | 84/100 [01:20<00:14,  1.12it/s] 85%|████████▌ | 85/100 [01:21<00:13,  1.10it/s]                                                 85%|████████▌ | 85/100 [01:21<00:13,  1.10it/s] 86%|████████▌ | 86/100 [01:22<00:12,  1.11it/s]                                                 86%|████████▌ | 86/100 [01:22<00:12,  1.11it/s] 87%|████████▋ | 87/100 [01:23<00:11,  1.11it/s]                                                 87%|████████▋ | 87/100 [01:23<00:11,  1.11it/s] 88%|████████▊ | 88/100 [01:24<00:10,  1.11it/s]                                                 88%|████████▊ | 88/100 [01:24<00:10,  1.11it/s] 89%|████████▉ | 89/100 [01:25<00:09,  1.11it/s]                                                 89%|████████▉ | 89/100 [01:25<00:09,  1.11it/s] 90%|█████████ | 90/100 [01:26<00:09,  1.11it/s]                                                 90%|█████████ | 90/100 [01:26<00:09,  1.11it/s] 91%|█████████ | 91/100 [01:27<00:08,  1.11it/s]                                                 91%|█████████ | 91/100 [01:27<00:08,  1.11it/s] 92%|█████████▏| 92/100 [01:28<00:07,  1.11it/s]                                                 92%|█████████▏| 92/100 [01:28<00:07,  1.11it/s] 93%|█████████▎| 93/100 [01:29<00:06,  1.12it/s]                                                 93%|█████████▎| 93/100 [01:29<00:06,  1.12it/s] 94%|█████████▍| 94/100 [01:29<00:05,  1.11it/s]                                                 94%|█████████▍| 94/100 [01:29<00:05,  1.11it/s] 95%|█████████▌| 95/100 [01:30<00:04,  1.10it/s]                                                 95%|█████████▌| 95/100 [01:30<00:04,  1.10it/s] 96%|█████████▌| 96/100 [01:31<00:03,  1.08it/s]                                                 96%|█████████▌| 96/100 [01:31<00:03,  1.08it/s] 97%|█████████▋| 97/100 [01:32<00:02,  1.08it/s]                                                 97%|█████████▋| 97/100 [01:32<00:02,  1.08it/s] 98%|█████████▊| 98/100 [01:33<00:01,  1.09it/s]                                                 98%|█████████▊| 98/100 [01:33<00:01,  1.09it/s] 99%|█████████▉| 99/100 [01:34<00:00,  1.08it/s]                                                 99%|█████████▉| 99/100 [01:34<00:00,  1.08it/s]100%|██████████| 100/100 [01:35<00:00,  1.07it/s]                                                 100%|██████████| 100/100 [01:35<00:00,  1.07it/s]                                                 100%|██████████| 100/100 [01:35<00:00,  1.07it/s]100%|██████████| 100/100 [01:35<00:00,  1.04it/s]
[rank1]:[W218 19:10:39.471397610 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W218 19:10:39.507342780 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
