/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|                                       | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|█▋                             | 1/19 [00:13<04:01, 13.41s/it]You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|                                       | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                       | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                       | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                       | 0/19 [00:00<?, ?it/s]You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|                                       | 0/19 [00:00<?, ?it/s]You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|                                       | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                       | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|█▋                             | 1/19 [00:13<04:10, 13.92s/it]Loading checkpoint shards:   5%|█▋                             | 1/19 [00:13<04:10, 13.92s/it]Loading checkpoint shards:   5%|█▋                             | 1/19 [00:13<04:10, 13.93s/it]Loading checkpoint shards:   5%|█▋                             | 1/19 [00:13<04:00, 13.39s/it]Loading checkpoint shards:   5%|█▋                             | 1/19 [00:13<04:01, 13.40s/it]Loading checkpoint shards:   5%|█▋                             | 1/19 [00:13<04:10, 13.94s/it]Loading checkpoint shards:  11%|███▎                           | 2/19 [00:29<04:19, 15.24s/it]Loading checkpoint shards:   5%|█▋                             | 1/19 [00:13<04:05, 13.65s/it]Loading checkpoint shards:  11%|███▎                           | 2/19 [00:14<01:39,  5.83s/it]Loading checkpoint shards:  11%|███▎                           | 2/19 [00:14<01:39,  5.86s/it]Loading checkpoint shards:  11%|███▎                           | 2/19 [00:13<01:35,  5.64s/it]Loading checkpoint shards:  11%|███▎                           | 2/19 [00:14<01:39,  5.87s/it]Loading checkpoint shards:  11%|███▎                           | 2/19 [00:13<01:35,  5.64s/it]Loading checkpoint shards:  11%|███▎                           | 2/19 [00:14<01:39,  5.87s/it]Loading checkpoint shards:  11%|███▎                           | 2/19 [00:14<01:45,  6.23s/it]Loading checkpoint shards:  16%|████▉                          | 3/19 [00:42<04:17, 16.10s/it]Loading checkpoint shards:  16%|████▉                          | 3/19 [00:42<04:17, 16.10s/it]Loading checkpoint shards:  16%|████▉                          | 3/19 [00:41<04:15, 15.98s/it]Loading checkpoint shards:  16%|████▉                          | 3/19 [00:42<04:17, 16.10s/it]Loading checkpoint shards:  16%|████▉                          | 3/19 [00:41<04:15, 15.98s/it]Loading checkpoint shards:  16%|████▉                          | 3/19 [00:42<04:17, 16.12s/it]Loading checkpoint shards:  16%|████▉                          | 3/19 [00:42<04:14, 15.90s/it]Loading checkpoint shards:  16%|████▉                          | 3/19 [00:58<05:40, 21.30s/it]Loading checkpoint shards:  21%|██████▌                        | 4/19 [00:56<03:51, 15.45s/it]Loading checkpoint shards:  21%|██████▌                        | 4/19 [00:56<03:51, 15.45s/it]Loading checkpoint shards:  21%|██████▌                        | 4/19 [00:56<03:51, 15.46s/it]Loading checkpoint shards:  21%|██████▌                        | 4/19 [00:56<03:51, 15.46s/it]Loading checkpoint shards:  21%|██████▌                        | 4/19 [00:56<03:50, 15.38s/it]Loading checkpoint shards:  21%|██████▌                        | 4/19 [00:56<03:49, 15.31s/it]Loading checkpoint shards:  21%|██████▌                        | 4/19 [00:56<03:51, 15.41s/it]Loading checkpoint shards:  21%|██████▌                        | 4/19 [01:12<04:39, 18.60s/it]Loading checkpoint shards:  26%|████████▏                      | 5/19 [01:23<04:31, 19.42s/it]Loading checkpoint shards:  26%|████████▏                      | 5/19 [01:23<04:31, 19.43s/it]Loading checkpoint shards:  26%|████████▏                      | 5/19 [01:22<04:30, 19.33s/it]Loading checkpoint shards:  26%|████████▏                      | 5/19 [01:22<04:31, 19.38s/it]Loading checkpoint shards:  26%|████████▏                      | 5/19 [01:23<04:31, 19.43s/it]Loading checkpoint shards:  26%|████████▏                      | 5/19 [01:39<04:59, 21.41s/it]Loading checkpoint shards:  26%|████████▏                      | 5/19 [01:23<04:32, 19.46s/it]Loading checkpoint shards:  26%|████████▏                      | 5/19 [01:22<04:31, 19.42s/it]Loading checkpoint shards:  32%|█████████▊                     | 6/19 [01:48<04:37, 21.34s/it]Loading checkpoint shards:  32%|█████████▊                     | 6/19 [01:48<04:36, 21.27s/it]Loading checkpoint shards:  32%|█████████▊                     | 6/19 [01:47<04:36, 21.29s/it]Loading checkpoint shards:  32%|█████████▊                     | 6/19 [01:48<04:37, 21.34s/it]Loading checkpoint shards:  32%|█████████▊                     | 6/19 [01:47<04:36, 21.31s/it]Loading checkpoint shards:  32%|█████████▊                     | 6/19 [01:48<04:37, 21.34s/it]Loading checkpoint shards:  32%|█████████▊                     | 6/19 [01:48<04:37, 21.34s/it]Loading checkpoint shards:  32%|█████████▊                     | 6/19 [02:04<04:54, 22.67s/it]Loading checkpoint shards:  37%|███████████▍                   | 7/19 [02:06<04:03, 20.27s/it]Loading checkpoint shards:  37%|███████████▍                   | 7/19 [02:05<04:02, 20.24s/it]Loading checkpoint shards:  37%|███████████▍                   | 7/19 [02:06<04:03, 20.28s/it]Loading checkpoint shards:  37%|███████████▍                   | 7/19 [02:06<04:02, 20.23s/it]Loading checkpoint shards:  37%|███████████▍                   | 7/19 [02:05<04:03, 20.25s/it]Loading checkpoint shards:  37%|███████████▍                   | 7/19 [02:06<04:03, 20.27s/it]Loading checkpoint shards:  37%|███████████▍                   | 7/19 [02:06<04:03, 20.27s/it]Loading checkpoint shards:  37%|███████████▍                   | 7/19 [02:22<04:14, 21.17s/it]Loading checkpoint shards:  42%|█████████████                  | 8/19 [02:25<03:39, 19.97s/it]Loading checkpoint shards:  42%|█████████████                  | 8/19 [02:25<03:39, 20.00s/it]Loading checkpoint shards:  42%|█████████████                  | 8/19 [02:25<03:39, 19.98s/it]Loading checkpoint shards:  42%|█████████████                  | 8/19 [02:25<03:40, 20.00s/it]Loading checkpoint shards:  42%|█████████████                  | 8/19 [02:25<03:39, 19.98s/it]Loading checkpoint shards:  42%|█████████████                  | 8/19 [02:25<03:39, 20.00s/it]Loading checkpoint shards:  42%|█████████████                  | 8/19 [02:41<03:46, 20.61s/it]Loading checkpoint shards:  42%|█████████████                  | 8/19 [02:25<03:40, 20.02s/it]Loading checkpoint shards:  47%|██████████████▋                | 9/19 [02:44<03:15, 19.53s/it]Loading checkpoint shards:  47%|██████████████▋                | 9/19 [02:44<03:15, 19.51s/it]Loading checkpoint shards:  47%|██████████████▋                | 9/19 [02:43<03:15, 19.52s/it]Loading checkpoint shards:  47%|██████████████▋                | 9/19 [02:44<03:15, 19.53s/it]Loading checkpoint shards:  47%|██████████████▋                | 9/19 [02:43<03:15, 19.52s/it]Loading checkpoint shards:  47%|██████████████▋                | 9/19 [02:44<03:15, 19.53s/it]Loading checkpoint shards:  47%|██████████████▋                | 9/19 [03:00<03:19, 19.94s/it]Loading checkpoint shards:  47%|██████████████▋                | 9/19 [02:44<03:15, 19.55s/it]Loading checkpoint shards:  53%|███████████████▊              | 10/19 [03:11<03:18, 22.05s/it]Loading checkpoint shards:  53%|███████████████▊              | 10/19 [03:12<03:18, 22.06s/it]Loading checkpoint shards:  53%|███████████████▊              | 10/19 [03:12<03:18, 22.07s/it]Loading checkpoint shards:  53%|███████████████▊              | 10/19 [03:12<03:18, 22.07s/it]Loading checkpoint shards:  53%|███████████████▊              | 10/19 [03:11<03:18, 22.06s/it]Loading checkpoint shards:  53%|███████████████▊              | 10/19 [03:11<03:18, 22.06s/it]Loading checkpoint shards:  53%|███████████████▊              | 10/19 [03:28<03:21, 22.36s/it]Loading checkpoint shards:  53%|███████████████▊              | 10/19 [03:12<03:18, 22.08s/it]Loading checkpoint shards:  58%|█████████████████▎            | 11/19 [03:40<03:12, 24.03s/it]Loading checkpoint shards:  58%|█████████████████▎            | 11/19 [03:40<03:12, 24.04s/it]Loading checkpoint shards:  58%|█████████████████▎            | 11/19 [03:40<03:12, 24.03s/it]Loading checkpoint shards:  58%|█████████████████▎            | 11/19 [03:40<03:12, 24.04s/it]Loading checkpoint shards:  58%|█████████████████▎            | 11/19 [03:40<03:12, 24.03s/it]Loading checkpoint shards:  58%|█████████████████▎            | 11/19 [03:56<03:13, 24.23s/it]Loading checkpoint shards:  58%|█████████████████▎            | 11/19 [03:40<03:12, 24.04s/it]Loading checkpoint shards:  58%|█████████████████▎            | 11/19 [03:40<03:12, 24.05s/it]Loading checkpoint shards:  63%|██████████████████▉           | 12/19 [04:00<02:41, 23.02s/it]Loading checkpoint shards:  63%|██████████████████▉           | 12/19 [04:01<02:41, 23.01s/it]Loading checkpoint shards:  63%|██████████████████▉           | 12/19 [04:01<02:41, 23.02s/it]Loading checkpoint shards:  63%|██████████████████▉           | 12/19 [04:01<02:41, 23.03s/it]Loading checkpoint shards:  63%|██████████████████▉           | 12/19 [04:01<02:41, 23.03s/it]Loading checkpoint shards:  63%|██████████████████▉           | 12/19 [04:00<02:41, 23.02s/it]Loading checkpoint shards:  63%|██████████████████▉           | 12/19 [04:00<02:41, 23.03s/it]Loading checkpoint shards:  63%|██████████████████▉           | 12/19 [04:17<02:42, 23.17s/it]Loading checkpoint shards:  68%|████████████████████▌         | 13/19 [04:30<02:29, 24.93s/it]Loading checkpoint shards:  68%|████████████████████▌         | 13/19 [04:30<02:29, 24.94s/it]Loading checkpoint shards:  68%|████████████████████▌         | 13/19 [04:30<02:29, 24.94s/it]Loading checkpoint shards:  68%|████████████████████▌         | 13/19 [04:30<02:29, 24.94s/it]Loading checkpoint shards:  68%|████████████████████▌         | 13/19 [04:30<02:29, 24.94s/it]Loading checkpoint shards:  68%|████████████████████▌         | 13/19 [04:30<02:29, 24.95s/it]Loading checkpoint shards:  68%|████████████████████▌         | 13/19 [04:30<02:29, 24.94s/it]Loading checkpoint shards:  68%|████████████████████▌         | 13/19 [04:46<02:30, 25.04s/it]Loading checkpoint shards:  74%|██████████████████████        | 14/19 [04:54<02:03, 24.70s/it]Loading checkpoint shards:  74%|██████████████████████        | 14/19 [04:54<02:03, 24.70s/it]Loading checkpoint shards:  74%|██████████████████████        | 14/19 [04:54<02:03, 24.70s/it]Loading checkpoint shards:  74%|██████████████████████        | 14/19 [04:54<02:03, 24.71s/it]Loading checkpoint shards:  74%|██████████████████████        | 14/19 [04:54<02:03, 24.70s/it]Loading checkpoint shards:  74%|██████████████████████        | 14/19 [04:54<02:03, 24.71s/it]Loading checkpoint shards:  74%|██████████████████████        | 14/19 [05:10<02:03, 24.77s/it]Loading checkpoint shards:  74%|██████████████████████        | 14/19 [04:54<02:03, 24.73s/it]Loading checkpoint shards:  79%|███████████████████████▋      | 15/19 [05:37<02:00, 30.08s/it]Loading checkpoint shards:  79%|███████████████████████▋      | 15/19 [05:37<02:00, 30.08s/it]Loading checkpoint shards:  79%|███████████████████████▋      | 15/19 [05:37<02:00, 30.09s/it]Loading checkpoint shards:  79%|███████████████████████▋      | 15/19 [05:36<02:00, 30.09s/it]Loading checkpoint shards:  79%|███████████████████████▋      | 15/19 [05:37<02:00, 30.08s/it]Loading checkpoint shards:  79%|███████████████████████▋      | 15/19 [05:37<02:00, 30.09s/it]Loading checkpoint shards:  79%|███████████████████████▋      | 15/19 [05:36<02:00, 30.09s/it]Loading checkpoint shards:  79%|███████████████████████▋      | 15/19 [05:53<02:00, 30.13s/it]Loading checkpoint shards:  84%|█████████████████████████▎    | 16/19 [06:00<01:23, 27.90s/it]Loading checkpoint shards:  84%|█████████████████████████▎    | 16/19 [06:00<01:23, 27.90s/it]Loading checkpoint shards:  84%|█████████████████████████▎    | 16/19 [05:59<01:23, 27.90s/it]Loading checkpoint shards:  84%|█████████████████████████▎    | 16/19 [05:59<01:23, 27.91s/it]Loading checkpoint shards:  84%|█████████████████████████▎    | 16/19 [06:00<01:23, 27.90s/it]Loading checkpoint shards:  84%|█████████████████████████▎    | 16/19 [05:59<01:23, 27.90s/it]Loading checkpoint shards:  84%|█████████████████████████▎    | 16/19 [06:16<01:23, 27.94s/it]Loading checkpoint shards:  84%|█████████████████████████▎    | 16/19 [06:00<01:23, 27.93s/it]Loading checkpoint shards:  89%|██████████████████████████▊   | 17/19 [06:25<00:54, 27.20s/it]Loading checkpoint shards:  89%|██████████████████████████▊   | 17/19 [06:25<00:54, 27.21s/it]Loading checkpoint shards:  89%|██████████████████████████▊   | 17/19 [06:25<00:54, 27.21s/it]Loading checkpoint shards:  89%|██████████████████████████▊   | 17/19 [06:25<00:54, 27.21s/it]Loading checkpoint shards:  89%|██████████████████████████▊   | 17/19 [06:25<00:54, 27.21s/it]Loading checkpoint shards:  89%|██████████████████████████▊   | 17/19 [06:25<00:54, 27.21s/it]Loading checkpoint shards:  89%|██████████████████████████▊   | 17/19 [06:41<00:54, 27.24s/it]Loading checkpoint shards:  89%|██████████████████████████▊   | 17/19 [06:25<00:54, 27.23s/it]Loading checkpoint shards:  95%|████████████████████████████▍ | 18/19 [06:46<00:25, 25.49s/it]Loading checkpoint shards:  95%|████████████████████████████▍ | 18/19 [06:47<00:25, 25.49s/it]Loading checkpoint shards:  95%|████████████████████████████▍ | 18/19 [06:47<00:25, 25.50s/it]Loading checkpoint shards:  95%|████████████████████████████▍ | 18/19 [06:47<00:25, 25.50s/it]Loading checkpoint shards:  95%|████████████████████████████▍ | 18/19 [06:46<00:25, 25.50s/it]Loading checkpoint shards:  95%|████████████████████████████▍ | 18/19 [06:46<00:25, 25.50s/it]Loading checkpoint shards:  95%|████████████████████████████▍ | 18/19 [07:03<00:25, 25.51s/it]Loading checkpoint shards:  95%|████████████████████████████▍ | 18/19 [06:47<00:25, 25.51s/it]Loading checkpoint shards: 100%|██████████████████████████████| 19/19 [07:01<00:00, 22.26s/it]Loading checkpoint shards: 100%|██████████████████████████████| 19/19 [07:01<00:00, 22.20s/it]
Loading checkpoint shards: 100%|██████████████████████████████| 19/19 [07:02<00:00, 22.27s/it]Loading checkpoint shards: 100%|██████████████████████████████| 19/19 [07:02<00:00, 22.22s/it]
Loading checkpoint shards: 100%|██████████████████████████████| 19/19 [07:02<00:00, 22.27s/it]Loading checkpoint shards: 100%|██████████████████████████████| 19/19 [07:02<00:00, 22.22s/it]
Loading checkpoint shards: 100%|██████████████████████████████| 19/19 [07:01<00:00, 22.27s/it]Loading checkpoint shards: 100%|██████████████████████████████| 19/19 [07:01<00:00, 22.19s/it]
Loading checkpoint shards: 100%|██████████████████████████████| 19/19 [07:01<00:00, 22.27s/it]Loading checkpoint shards: 100%|██████████████████████████████| 19/19 [07:02<00:00, 22.27s/it]Loading checkpoint shards: 100%|██████████████████████████████| 19/19 [07:01<00:00, 22.19s/it]
Loading checkpoint shards: 100%|██████████████████████████████| 19/19 [07:02<00:00, 22.22s/it]
Loading checkpoint shards: 100%|██████████████████████████████| 19/19 [07:18<00:00, 22.28s/it]Loading checkpoint shards: 100%|██████████████████████████████| 19/19 [07:18<00:00, 23.06s/it]
Loading checkpoint shards: 100%|██████████████████████████████| 19/19 [07:02<00:00, 22.28s/it]Loading checkpoint shards: 100%|██████████████████████████████| 19/19 [07:02<00:00, 22.22s/it]
WARNING:sft_trainer.py:PAD token set to default, missing in tokenizer
WARNING:sft_trainer.py:PAD token set to default, missing in tokenizer
WARNING:sft_trainer.py:PAD token set to default, missing in tokenizer
WARNING:sft_trainer.py:PAD token set to default, missing in tokenizer
WARNING:sft_trainer.py:PAD token set to default, missing in tokenizer
WARNING:sft_trainer.py:PAD token set to default, missing in tokenizer
WARNING:sft_trainer.py:PAD token set to default, missing in tokenizer
WARNING:sft_trainer.py:PAD token set to default, missing in tokenizer
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
You are using a model of type mixtral to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
Converting ScatterMoE layers:   0%|                                    | 0/32 [00:00<?, ?it/s]Converting ScatterMoE layers:   3%|▉                           | 1/32 [00:04<02:20,  4.53s/it]Converting ScatterMoE layers:   6%|█▊                          | 2/32 [00:10<02:40,  5.34s/it]Converting ScatterMoE layers:   9%|██▋                         | 3/32 [00:13<02:04,  4.29s/it]Converting ScatterMoE layers:  12%|███▌                        | 4/32 [00:17<01:58,  4.22s/it]Converting ScatterMoE layers:  16%|████▍                       | 5/32 [00:21<01:50,  4.08s/it]Converting ScatterMoE layers:  19%|█████▎                      | 6/32 [00:24<01:40,  3.87s/it]Converting ScatterMoE layers:  22%|██████▏                     | 7/32 [00:28<01:33,  3.73s/it]Converting ScatterMoE layers:  25%|███████                     | 8/32 [00:32<01:32,  3.84s/it]You are using a model of type mixtral to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
You are using a model of type mixtral to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
You are using a model of type mixtral to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
You are using a model of type mixtral to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
You are using a model of type mixtral to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
You are using a model of type mixtral to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
You are using a model of type mixtral to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
Converting ScatterMoE layers:  28%|███████▉                    | 9/32 [01:05<05:00, 13.07s/it]Converting ScatterMoE layers:  31%|████████▍                  | 10/32 [01:21<05:03, 13.78s/it]Converting ScatterMoE layers:  34%|█████████▎                 | 11/32 [01:45<05:56, 16.96s/it]Converting ScatterMoE layers:  38%|██████████▏                | 12/32 [02:00<05:28, 16.43s/it]Converting ScatterMoE layers:  41%|██████████▉                | 13/32 [02:27<06:12, 19.61s/it]Converting ScatterMoE layers:  44%|███████████▊               | 14/32 [02:51<06:14, 20.83s/it]Converting ScatterMoE layers:  47%|████████████▋              | 15/32 [03:17<06:24, 22.63s/it]Converting ScatterMoE layers:  50%|█████████████▌             | 16/32 [03:47<06:37, 24.82s/it]Converting ScatterMoE layers:  53%|██████████████▎            | 17/32 [04:21<06:51, 27.40s/it]Converting ScatterMoE layers:  56%|███████████████▏           | 18/32 [04:39<05:44, 24.57s/it]Converting ScatterMoE layers:  59%|████████████████           | 19/32 [05:01<05:12, 24.01s/it]Converting ScatterMoE layers:  62%|████████████████▉          | 20/32 [05:31<05:09, 25.81s/it]Converting ScatterMoE layers:  66%|█████████████████▋         | 21/32 [05:52<04:27, 24.27s/it]Converting ScatterMoE layers:  69%|██████████████████▌        | 22/32 [06:11<03:45, 22.56s/it]Converting ScatterMoE layers:  72%|███████████████████▍       | 23/32 [06:27<03:07, 20.80s/it]Converting ScatterMoE layers:  75%|████████████████████▎      | 24/32 [06:54<03:00, 22.52s/it]Converting ScatterMoE layers:  78%|█████████████████████      | 25/32 [07:22<02:49, 24.26s/it]Converting ScatterMoE layers:  81%|█████████████████████▉     | 26/32 [07:53<02:36, 26.15s/it]Converting ScatterMoE layers:  84%|██████████████████████▊    | 27/32 [08:22<02:15, 27.14s/it]Converting ScatterMoE layers:  88%|███████████████████████▋   | 28/32 [09:09<02:11, 32.98s/it]Converting ScatterMoE layers:  91%|████████████████████████▍  | 29/32 [09:32<01:30, 30.00s/it]Converting ScatterMoE layers:  94%|█████████████████████████▎ | 30/32 [09:57<00:57, 28.52s/it]Converting ScatterMoE layers:  97%|██████████████████████████▏| 31/32 [10:22<00:27, 27.39s/it]Converting ScatterMoE layers: 100%|███████████████████████████| 32/32 [10:46<00:00, 26.60s/it]Converting ScatterMoE layers: 100%|███████████████████████████| 32/32 [10:46<00:00, 20.22s/it]
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/tuning/sft_trainer.py:362: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  trainer = SFTTrainer(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/tuning/sft_trainer.py:362: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  trainer = SFTTrainer(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/tuning/sft_trainer.py:362: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  trainer = SFTTrainer(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/tuning/sft_trainer.py:362: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  trainer = SFTTrainer(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/tuning/sft_trainer.py:362: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  trainer = SFTTrainer(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/tuning/sft_trainer.py:362: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  trainer = SFTTrainer(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/tuning/sft_trainer.py:362: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  trainer = SFTTrainer(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:2077: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/tuning/sft_trainer.py:362: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  trainer = SFTTrainer(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.
  warnings.warn(
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.
  warnings.warn(
  0%|                                                                 | 0/100 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISMTOKENIZERS_PARALLELISM=(true | false)
=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
  1%|▌                                                        | 1/100 [00:21<35:41, 21.63s/it]                                                                                                1%|▌                                                        | 1/100 [00:21<35:41, 21.63s/it]  2%|█▏                                                       | 2/100 [00:28<21:30, 13.17s/it]                                                                                                2%|█▏                                                       | 2/100 [00:28<21:30, 13.17s/it]  3%|█▋                                                       | 3/100 [00:36<16:59, 10.51s/it]                                                                                                3%|█▋                                                       | 3/100 [00:36<16:59, 10.51s/it]  4%|██▎                                                      | 4/100 [00:43<14:41,  9.18s/it]                                                                                                4%|██▎                                                      | 4/100 [00:43<14:41,  9.18s/it]  5%|██▊                                                      | 5/100 [00:50<13:23,  8.46s/it]                                                                                                5%|██▊                                                      | 5/100 [00:50<13:23,  8.46s/it]  6%|███▍                                                     | 6/100 [00:57<12:37,  8.05s/it]                                                                                                6%|███▍                                                     | 6/100 [00:57<12:37,  8.05s/it]  7%|███▉                                                     | 7/100 [01:04<12:01,  7.76s/it]                                                                                                7%|███▉                                                     | 7/100 [01:05<12:01,  7.76s/it]  8%|████▌                                                    | 8/100 [01:12<11:38,  7.59s/it]                                                                                                8%|████▌                                                    | 8/100 [01:12<11:38,  7.59s/it]  9%|█████▏                                                   | 9/100 [01:19<11:19,  7.47s/it]                                                                                                9%|█████▏                                                   | 9/100 [01:19<11:19,  7.47s/it] 10%|█████▌                                                  | 10/100 [01:26<11:04,  7.38s/it]                                                                                               10%|█████▌                                                  | 10/100 [01:26<11:04,  7.38s/it] 11%|██████▏                                                 | 11/100 [01:33<10:53,  7.35s/it]                                                                                               11%|██████▏                                                 | 11/100 [01:33<10:53,  7.35s/it] 12%|██████▋                                                 | 12/100 [01:41<10:42,  7.30s/it]                                                                                               12%|██████▋                                                 | 12/100 [01:41<10:42,  7.30s/it] 13%|███████▎                                                | 13/100 [01:48<10:32,  7.27s/it]                                                                                               13%|███████▎                                                | 13/100 [01:48<10:32,  7.27s/it] 14%|███████▊                                                | 14/100 [01:55<10:24,  7.26s/it]                                                                                               14%|███████▊                                                | 14/100 [01:55<10:24,  7.26s/it] 15%|████████▍                                               | 15/100 [02:02<10:15,  7.25s/it]                                                                                               15%|████████▍                                               | 15/100 [02:02<10:15,  7.25s/it] 16%|████████▉                                               | 16/100 [02:09<10:05,  7.21s/it]                                                                                               16%|████████▉                                               | 16/100 [02:09<10:05,  7.21s/it] 17%|█████████▌                                              | 17/100 [02:17<10:00,  7.23s/it]                                                                                               17%|█████████▌                                              | 17/100 [02:17<10:00,  7.23s/it] 18%|██████████                                              | 18/100 [02:24<09:50,  7.20s/it]                                                                                               18%|██████████                                              | 18/100 [02:24<09:50,  7.20s/it] 19%|██████████▋                                             | 19/100 [02:31<09:45,  7.23s/it]                                                                                               19%|██████████▋                                             | 19/100 [02:31<09:45,  7.23s/it] 20%|███████████▏                                            | 20/100 [02:38<09:36,  7.21s/it]                                                                                               20%|███████████▏                                            | 20/100 [02:38<09:36,  7.21s/it] 21%|███████████▊                                            | 21/100 [02:45<09:26,  7.17s/it]                                                                                               21%|███████████▊                                            | 21/100 [02:45<09:26,  7.17s/it] 22%|████████████▎                                           | 22/100 [02:52<09:18,  7.16s/it]                                                                                               22%|████████████▎                                           | 22/100 [02:53<09:18,  7.16s/it] 23%|████████████▉                                           | 23/100 [03:00<09:13,  7.19s/it]                                                                                               23%|████████████▉                                           | 23/100 [03:00<09:13,  7.19s/it] 24%|█████████████▍                                          | 24/100 [03:07<09:10,  7.24s/it]                                                                                               24%|█████████████▍                                          | 24/100 [03:07<09:10,  7.24s/it] 25%|██████████████                                          | 25/100 [03:14<09:02,  7.24s/it]                                                                                               25%|██████████████                                          | 25/100 [03:14<09:02,  7.24s/it] 26%|██████████████▌                                         | 26/100 [03:21<08:53,  7.21s/it]                                                                                               26%|██████████████▌                                         | 26/100 [03:22<08:53,  7.21s/it] 27%|███████████████                                         | 27/100 [03:29<08:46,  7.22s/it]                                                                                               27%|███████████████                                         | 27/100 [03:29<08:46,  7.22s/it] 28%|███████████████▋                                        | 28/100 [03:36<08:39,  7.22s/it]                                                                                               28%|███████████████▋                                        | 28/100 [03:36<08:39,  7.22s/it] 29%|████████████████▏                                       | 29/100 [03:43<08:31,  7.20s/it]                                                                                               29%|████████████████▏                                       | 29/100 [03:43<08:31,  7.20s/it] 30%|████████████████▊                                       | 30/100 [03:50<08:24,  7.20s/it]                                                                                               30%|████████████████▊                                       | 30/100 [03:50<08:24,  7.20s/it] 31%|█████████████████▎                                      | 31/100 [03:57<08:14,  7.17s/it]                                                                                               31%|█████████████████▎                                      | 31/100 [03:57<08:14,  7.17s/it] 32%|█████████████████▉                                      | 32/100 [04:04<08:06,  7.15s/it]                                                                                               32%|█████████████████▉                                      | 32/100 [04:05<08:06,  7.15s/it] 33%|██████████████████▍                                     | 33/100 [04:12<08:01,  7.19s/it]                                                                                               33%|██████████████████▍                                     | 33/100 [04:12<08:01,  7.19s/it] 34%|███████████████████                                     | 34/100 [04:19<07:54,  7.19s/it]                                                                                               34%|███████████████████                                     | 34/100 [04:19<07:54,  7.19s/it] 35%|███████████████████▌                                    | 35/100 [04:26<07:46,  7.18s/it]                                                                                               35%|███████████████████▌                                    | 35/100 [04:26<07:46,  7.18s/it] 36%|████████████████████▏                                   | 36/100 [04:33<07:38,  7.16s/it]                                                                                               36%|████████████████████▏                                   | 36/100 [04:33<07:38,  7.16s/it] 37%|████████████████████▋                                   | 37/100 [04:40<07:32,  7.18s/it]                                                                                               37%|████████████████████▋                                   | 37/100 [04:41<07:32,  7.18s/it] 38%|█████████████████████▎                                  | 38/100 [04:48<07:26,  7.20s/it]                                                                                               38%|█████████████████████▎                                  | 38/100 [04:48<07:26,  7.20s/it] 39%|█████████████████████▊                                  | 39/100 [04:55<07:20,  7.22s/it]                                                                                               39%|█████████████████████▊                                  | 39/100 [04:55<07:20,  7.22s/it] 40%|██████████████████████▍                                 | 40/100 [05:02<07:10,  7.17s/it]                                                                                               40%|██████████████████████▍                                 | 40/100 [05:02<07:10,  7.17s/it] 41%|██████████████████████▉                                 | 41/100 [05:09<07:01,  7.14s/it]                                                                                               41%|██████████████████████▉                                 | 41/100 [05:09<07:01,  7.14s/it] 42%|███████████████████████▌                                | 42/100 [05:16<06:55,  7.17s/it]                                                                                               42%|███████████████████████▌                                | 42/100 [05:16<06:55,  7.17s/it] 43%|████████████████████████                                | 43/100 [05:23<06:47,  7.15s/it]                                                                                               43%|████████████████████████                                | 43/100 [05:24<06:47,  7.15s/it] 44%|████████████████████████▋                               | 44/100 [05:31<06:40,  7.16s/it]                                                                                               44%|████████████████████████▋                               | 44/100 [05:31<06:40,  7.16s/it] 45%|█████████████████████████▏                              | 45/100 [05:38<06:33,  7.16s/it]                                                                                               45%|█████████████████████████▏                              | 45/100 [05:38<06:33,  7.16s/it] 46%|█████████████████████████▊                              | 46/100 [05:45<06:25,  7.14s/it]                                                                                               46%|█████████████████████████▊                              | 46/100 [05:45<06:25,  7.14s/it] 47%|██████████████████████████▎                             | 47/100 [05:52<06:18,  7.14s/it]                                                                                               47%|██████████████████████████▎                             | 47/100 [05:52<06:18,  7.14s/it] 48%|██████████████████████████▉                             | 48/100 [05:59<06:11,  7.14s/it]                                                                                               48%|██████████████████████████▉                             | 48/100 [05:59<06:11,  7.14s/it] 49%|███████████████████████████▍                            | 49/100 [06:06<06:04,  7.14s/it]                                                                                               49%|███████████████████████████▍                            | 49/100 [06:06<06:04,  7.14s/it] 50%|████████████████████████████                            | 50/100 [06:13<05:55,  7.12s/it]                                                                                               50%|████████████████████████████                            | 50/100 [06:13<05:55,  7.12s/it] 51%|████████████████████████████▌                           | 51/100 [06:21<05:50,  7.15s/it]                                                                                               51%|████████████████████████████▌                           | 51/100 [06:21<05:50,  7.15s/it] 52%|█████████████████████████████                           | 52/100 [06:28<05:43,  7.15s/it]                                                                                               52%|█████████████████████████████                           | 52/100 [06:28<05:43,  7.15s/it] 53%|█████████████████████████████▋                          | 53/100 [06:35<05:36,  7.16s/it]                                                                                               53%|█████████████████████████████▋                          | 53/100 [06:35<05:36,  7.16s/it] 54%|██████████████████████████████▏                         | 54/100 [06:42<05:29,  7.17s/it]                                                                                               54%|██████████████████████████████▏                         | 54/100 [06:42<05:29,  7.17s/it] 55%|██████████████████████████████▊                         | 55/100 [06:49<05:24,  7.20s/it]                                                                                               55%|██████████████████████████████▊                         | 55/100 [06:49<05:24,  7.20s/it] 56%|███████████████████████████████▎                        | 56/100 [06:56<05:15,  7.18s/it]                                                                                               56%|███████████████████████████████▎                        | 56/100 [06:57<05:15,  7.18s/it] 57%|███████████████████████████████▉                        | 57/100 [07:04<05:08,  7.18s/it]                                                                                               57%|███████████████████████████████▉                        | 57/100 [07:04<05:08,  7.18s/it] 58%|████████████████████████████████▍                       | 58/100 [07:11<05:04,  7.25s/it]                                                                                               58%|████████████████████████████████▍                       | 58/100 [07:11<05:04,  7.25s/it] 59%|█████████████████████████████████                       | 59/100 [07:19<05:02,  7.38s/it]                                                                                               59%|█████████████████████████████████                       | 59/100 [07:19<05:02,  7.38s/it] 60%|█████████████████████████████████▌                      | 60/100 [07:26<04:54,  7.37s/it]                                                                                               60%|█████████████████████████████████▌                      | 60/100 [07:26<04:54,  7.37s/it] 61%|██████████████████████████████████▏                     | 61/100 [07:33<04:45,  7.33s/it]                                                                                               61%|██████████████████████████████████▏                     | 61/100 [07:33<04:45,  7.33s/it] 62%|██████████████████████████████████▋                     | 62/100 [07:41<04:38,  7.33s/it]                                                                                               62%|██████████████████████████████████▋                     | 62/100 [07:41<04:38,  7.33s/it] 63%|███████████████████████████████████▎                    | 63/100 [07:48<04:28,  7.27s/it]                                                                                               63%|███████████████████████████████████▎                    | 63/100 [07:48<04:28,  7.27s/it] 64%|███████████████████████████████████▊                    | 64/100 [07:55<04:20,  7.23s/it]                                                                                               64%|███████████████████████████████████▊                    | 64/100 [07:55<04:20,  7.23s/it] 65%|████████████████████████████████████▍                   | 65/100 [08:02<04:12,  7.23s/it]                                                                                               65%|████████████████████████████████████▍                   | 65/100 [08:02<04:12,  7.23s/it] 66%|████████████████████████████████████▉                   | 66/100 [08:10<04:07,  7.26s/it]                                                                                               66%|████████████████████████████████████▉                   | 66/100 [08:10<04:07,  7.26s/it] 67%|█████████████████████████████████████▌                  | 67/100 [08:17<03:59,  7.25s/it]                                                                                               67%|█████████████████████████████████████▌                  | 67/100 [08:17<03:59,  7.25s/it] 68%|██████████████████████████████████████                  | 68/100 [08:24<03:51,  7.22s/it]                                                                                               68%|██████████████████████████████████████                  | 68/100 [08:24<03:51,  7.22s/it] 69%|██████████████████████████████████████▋                 | 69/100 [08:31<03:44,  7.23s/it]                                                                                               69%|██████████████████████████████████████▋                 | 69/100 [08:31<03:44,  7.23s/it] 70%|███████████████████████████████████████▏                | 70/100 [08:38<03:36,  7.23s/it]                                                                                               70%|███████████████████████████████████████▏                | 70/100 [08:38<03:36,  7.23s/it] 71%|███████████████████████████████████████▊                | 71/100 [08:46<03:29,  7.21s/it]                                                                                               71%|███████████████████████████████████████▊                | 71/100 [08:46<03:29,  7.21s/it] 72%|████████████████████████████████████████▎               | 72/100 [08:53<03:23,  7.27s/it]                                                                                               72%|████████████████████████████████████████▎               | 72/100 [08:53<03:23,  7.27s/it] 73%|████████████████████████████████████████▉               | 73/100 [09:00<03:15,  7.24s/it]                                                                                               73%|████████████████████████████████████████▉               | 73/100 [09:00<03:15,  7.24s/it] 74%|█████████████████████████████████████████▍              | 74/100 [09:07<03:08,  7.23s/it]                                                                                               74%|█████████████████████████████████████████▍              | 74/100 [09:07<03:08,  7.23s/it] 75%|██████████████████████████████████████████              | 75/100 [09:15<03:01,  7.25s/it]                                                                                               75%|██████████████████████████████████████████              | 75/100 [09:15<03:01,  7.25s/it] 76%|██████████████████████████████████████████▌             | 76/100 [09:22<02:53,  7.22s/it]                                                                                               76%|██████████████████████████████████████████▌             | 76/100 [09:22<02:53,  7.22s/it] 77%|███████████████████████████████████████████             | 77/100 [09:29<02:45,  7.19s/it]                                                                                               77%|███████████████████████████████████████████             | 77/100 [09:29<02:45,  7.19s/it] 78%|███████████████████████████████████████████▋            | 78/100 [09:36<02:37,  7.17s/it]                                                                                               78%|███████████████████████████████████████████▋            | 78/100 [09:36<02:37,  7.17s/it] 79%|████████████████████████████████████████████▏           | 79/100 [09:43<02:30,  7.15s/it]                                                                                               79%|████████████████████████████████████████████▏           | 79/100 [09:43<02:30,  7.15s/it] 80%|████████████████████████████████████████████▊           | 80/100 [09:50<02:23,  7.16s/it]                                                                                               80%|████████████████████████████████████████████▊           | 80/100 [09:50<02:23,  7.16s/it] 81%|█████████████████████████████████████████████▎          | 81/100 [09:57<02:15,  7.14s/it]                                                                                               81%|█████████████████████████████████████████████▎          | 81/100 [09:58<02:15,  7.14s/it] 82%|█████████████████████████████████████████████▉          | 82/100 [10:05<02:08,  7.16s/it]                                                                                               82%|█████████████████████████████████████████████▉          | 82/100 [10:05<02:08,  7.16s/it] 83%|██████████████████████████████████████████████▍         | 83/100 [10:12<02:01,  7.15s/it]                                                                                               83%|██████████████████████████████████████████████▍         | 83/100 [10:12<02:01,  7.15s/it] 84%|███████████████████████████████████████████████         | 84/100 [10:19<01:55,  7.20s/it]                                                                                               84%|███████████████████████████████████████████████         | 84/100 [10:19<01:55,  7.20s/it] 85%|███████████████████████████████████████████████▌        | 85/100 [10:26<01:47,  7.19s/it]                                                                                               85%|███████████████████████████████████████████████▌        | 85/100 [10:26<01:47,  7.19s/it] 86%|████████████████████████████████████████████████▏       | 86/100 [10:33<01:40,  7.17s/it]                                                                                               86%|████████████████████████████████████████████████▏       | 86/100 [10:33<01:40,  7.17s/it] 87%|████████████████████████████████████████████████▋       | 87/100 [10:41<01:33,  7.20s/it]                                                                                               87%|████████████████████████████████████████████████▋       | 87/100 [10:41<01:33,  7.20s/it] 88%|█████████████████████████████████████████████████▎      | 88/100 [10:48<01:26,  7.19s/it]                                                                                               88%|█████████████████████████████████████████████████▎      | 88/100 [10:48<01:26,  7.19s/it] 89%|█████████████████████████████████████████████████▊      | 89/100 [10:55<01:19,  7.19s/it]                                                                                               89%|█████████████████████████████████████████████████▊      | 89/100 [10:55<01:19,  7.19s/it] 90%|██████████████████████████████████████████████████▍     | 90/100 [11:02<01:11,  7.20s/it]                                                                                               90%|██████████████████████████████████████████████████▍     | 90/100 [11:02<01:11,  7.20s/it] 91%|██████████████████████████████████████████████████▉     | 91/100 [11:09<01:04,  7.19s/it]                                                                                               91%|██████████████████████████████████████████████████▉     | 91/100 [11:09<01:04,  7.19s/it] 92%|███████████████████████████████████████████████████▌    | 92/100 [11:17<00:57,  7.18s/it]                                                                                               92%|███████████████████████████████████████████████████▌    | 92/100 [11:17<00:57,  7.18s/it] 93%|████████████████████████████████████████████████████    | 93/100 [11:24<00:50,  7.15s/it]                                                                                               93%|████████████████████████████████████████████████████    | 93/100 [11:24<00:50,  7.15s/it] 94%|████████████████████████████████████████████████████▋   | 94/100 [11:31<00:42,  7.12s/it]                                                                                               94%|████████████████████████████████████████████████████▋   | 94/100 [11:31<00:42,  7.12s/it] 95%|█████████████████████████████████████████████████████▏  | 95/100 [11:38<00:35,  7.12s/it]                                                                                               95%|█████████████████████████████████████████████████████▏  | 95/100 [11:38<00:35,  7.12s/it] 96%|█████████████████████████████████████████████████████▊  | 96/100 [11:45<00:28,  7.14s/it]                                                                                               96%|█████████████████████████████████████████████████████▊  | 96/100 [11:45<00:28,  7.14s/it] 97%|██████████████████████████████████████████████████████▎ | 97/100 [11:52<00:21,  7.12s/it]                                                                                               97%|██████████████████████████████████████████████████████▎ | 97/100 [11:52<00:21,  7.12s/it] 98%|██████████████████████████████████████████████████████▉ | 98/100 [11:59<00:14,  7.14s/it]                                                                                               98%|██████████████████████████████████████████████████████▉ | 98/100 [11:59<00:14,  7.14s/it] 99%|███████████████████████████████████████████████████████▍| 99/100 [12:06<00:07,  7.16s/it]                                                                                               99%|███████████████████████████████████████████████████████▍| 99/100 [12:07<00:07,  7.16s/it]100%|███████████████████████████████████████████████████████| 100/100 [12:13<00:00,  7.13s/it]                                                                                              100%|███████████████████████████████████████████████████████| 100/100 [12:14<00:00,  7.13s/it]                                                                                              100%|███████████████████████████████████████████████████████| 100/100 [12:14<00:00,  7.13s/it]100%|███████████████████████████████████████████████████████| 100/100 [12:14<00:00,  7.34s/it]
[rank0]:[W220 18:38:16.547389028 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
