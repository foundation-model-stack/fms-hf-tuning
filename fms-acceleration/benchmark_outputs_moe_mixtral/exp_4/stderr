/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|                                       | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|█▋                             | 1/19 [00:10<03:05, 10.31s/it]Loading checkpoint shards:  11%|███▎                           | 2/19 [00:22<03:14, 11.45s/it]Loading checkpoint shards:  16%|████▉                          | 3/19 [00:36<03:17, 12.36s/it]Loading checkpoint shards:  21%|██████▌                        | 4/19 [00:48<03:04, 12.31s/it]Loading checkpoint shards:  26%|████████▏                      | 5/19 [01:03<03:08, 13.49s/it]Loading checkpoint shards:  32%|█████████▊                     | 6/19 [01:22<03:18, 15.29s/it]Loading checkpoint shards:  37%|███████████▍                   | 7/19 [01:38<03:06, 15.55s/it]Loading checkpoint shards:  42%|█████████████                  | 8/19 [01:55<02:55, 15.95s/it]Loading checkpoint shards:  47%|██████████████▋                | 9/19 [02:11<02:38, 15.83s/it]Loading checkpoint shards:  53%|███████████████▊              | 10/19 [02:30<02:31, 16.84s/it]Loading checkpoint shards:  58%|█████████████████▎            | 11/19 [02:46<02:14, 16.82s/it]Loading checkpoint shards:  63%|██████████████████▉           | 12/19 [03:02<01:55, 16.55s/it]Loading checkpoint shards:  68%|████████████████████▌         | 13/19 [03:18<01:38, 16.40s/it]Loading checkpoint shards:  74%|██████████████████████        | 14/19 [03:33<01:19, 15.84s/it]Loading checkpoint shards:  79%|███████████████████████▋      | 15/19 [03:49<01:03, 15.77s/it]Loading checkpoint shards:  84%|█████████████████████████▎    | 16/19 [04:00<00:43, 14.52s/it]Loading checkpoint shards:  89%|██████████████████████████▊   | 17/19 [04:13<00:27, 13.87s/it]Loading checkpoint shards:  95%|████████████████████████████▍ | 18/19 [04:23<00:12, 12.90s/it]Loading checkpoint shards: 100%|██████████████████████████████| 19/19 [04:33<00:00, 11.93s/it]Loading checkpoint shards: 100%|██████████████████████████████| 19/19 [04:33<00:00, 14.39s/it]
WARNING:sft_trainer.py:PAD token set to default, missing in tokenizer
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
ERROR:sft_trainer.py:Traceback (most recent call last):
  File "/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/tuning/sft_trainer.py", line 674, in main
    trainer, additional_train_info = train(
                                     ^^^^^^
  File "/workspace/fms-acceleration/.tox/run-benches/lib/python3.12/site-packages/tuning/sft_trainer.py", line 335, in train
    model, (peft_config,) = framework.augmentation(
                            ^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/fms-acceleration/plugins/framework/src/fms_acceleration/framework.py", line 206, in augmentation
    model, modifiable_args = plugin.augmentation(
                             ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/fms-acceleration/plugins/accelerated-moe/src/fms_acceleration_moe/framework_plugin_scattermoe.py", line 81, in augmentation
    self._moe_component_module_names = prepare_scattermoe(
                                       ^^^^^^^^^^^^^^^^^^^
  File "/workspace/fms-acceleration/plugins/accelerated-moe/src/fms_acceleration_moe/utils/scattermoe_prepare.py", line 121, in prepare_scattermoe
    assert world_size % ep_degree == 0, (
AssertionError: world size (1) not divisible by ep_size (8).

