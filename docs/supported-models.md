# Supported models list

- Legend:

  âœ… Ready and available 

  âœ”ï¸ Ready and available - compatible architecture (*see first bullet point above)

  ğŸš« Not supported

  ? May be supported, but not tested

Model Name & Size  | Model Architecture | Full Finetuning | Low Rank Adaptation (i.e. LoRA) | qLoRA(quantized LoRA) | 
-------------------- | ---------------- | --------------- | ------------------------------- | --------------------- |
[Granite 4.0 Tiny Preview](https://huggingface.co/ibm-granite/granite-4.0-tiny-preview) | GraniteMoeHybridForCausalLM | âœ… | âœ… | ? |
[Granite PowerLM 3B](https://huggingface.co/ibm-research/PowerLM-3b) | GraniteForCausalLM | âœ… | âœ… | âœ… |
[Granite 3.1 1B](https://huggingface.co/ibm-granite/granite-3.1-1b-a400m-base)       | GraniteForCausalLM | âœ”ï¸ | âœ”ï¸ | âœ”ï¸ |
[Granite 3.1 2B](https://huggingface.co/ibm-granite/granite-3.1-2b-base)             | GraniteForCausalLM | âœ”ï¸ | âœ”ï¸ | âœ”ï¸ |
[Granite 3.1 8B](https://huggingface.co/ibm-granite/granite-3.1-8b-base)       | GraniteForCausalLM | âœ”ï¸ | âœ”ï¸ | âœ”ï¸ |
[Granite 3.0 2B](https://huggingface.co/ibm-granite/granite-3.0-2b-base)       | GraniteForCausalLM | âœ”ï¸ | âœ”ï¸ | âœ”ï¸ |
[Granite 3.0 8B](https://huggingface.co/ibm-granite/granite-3.0-8b-base)       | GraniteForCausalLM | âœ… | âœ… | âœ”ï¸ |
[GraniteMoE 1B](https://huggingface.co/ibm-granite/granite-3.0-1b-a400m-base)        | GraniteMoeForCausalLM  | âœ… | âœ…* | ? |
[GraniteMoE 3B](https://huggingface.co/ibm-granite/granite-3.0-3b-a800m-base)        | GraniteMoeForCausalLM  | âœ… | âœ…* | ? |
[Granite 3B Code](https://huggingface.co/ibm-granite/granite-3b-code-base-2k)           | LlamaForCausalLM      | âœ… | âœ”ï¸  | âœ”ï¸ | 
[Granite 8B Code](https://huggingface.co/ibm-granite/granite-8b-code-base-4k)           | LlamaForCausalLM      | âœ… | âœ… | âœ… |
Granite 13B          | GPTBigCodeForCausalLM  | âœ… | âœ… | âœ”ï¸  | 
Granite 20B          | GPTBigCodeForCausalLM  | âœ… | âœ”ï¸  | âœ”ï¸  | 
[Granite 34B Code](https://huggingface.co/ibm-granite/granite-34b-code-instruct-8k)            | GPTBigCodeForCausalLM  | ğŸš« | âœ… | âœ… | 
[Llama3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B)          | LlamaForCausalLM               | âœ…** | âœ”ï¸ | âœ”ï¸ | Â 
[Llama3.1-70B](https://huggingface.co/meta-llama/Llama-3.1-70B)(same architecture as llama3) | LlamaForCausalLM   | ğŸš« - same as Llama3-70B | âœ”ï¸  | âœ”ï¸ | 
[Llama3.1-405B](https://huggingface.co/meta-llama/Llama-3.1-405B)                            | LlamaForCausalLM   | ğŸš« | ğŸš« | âœ… | 
[Llama3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)                               | LlamaForCausalLM   | âœ… | âœ… | âœ”ï¸ | Â 
[Llama3-70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B)                             | LlamaForCausalLM   | ğŸš« | âœ… | âœ… |
aLLaM-13b                                 | LlamaForCausalLM | Â âœ… | âœ… | âœ… |
[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)                              | MixtralForCausalLM   | âœ… | âœ… | âœ… |
[Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1)                                  | MistralForCausalLM   | âœ… | âœ… | âœ… | Â 
Mistral large                             | MistralForCausalLM   | ğŸš« | ğŸš« | ğŸš« | 
[GPT-OSS-20B](https://huggingface.co/openai/gpt-oss-20b)                                  | GptOssForCausalLM   | âœ… | âœ… | ? | Â 
[GPT-OSS-120B](https://huggingface.co/openai/gpt-oss-120b)                                  | GptOssForCausalLM   | âœ… | âœ… | ? | Â 

(*) - Supported for q,k,v,o layers . `all-linear` target modules does not infer on vLLM yet.

(**) - Supported from platform up to 8k context length - same architecture as llama3-8b.

### Supported vision model

We also support full fine-tuning and LoRA tuning for vision language models - `Granite 3.2 Vision`, `Llama 3.2 Vision`, and `LLaVa-Next` from `v2.8.1` onwards.
For information on supported dataset formats and how to tune a vision-language model, please see [this document](./vision-language-model-tuning.md).

Model Name & Size  | Model Architecture | LoRA Tuning | Full Finetuning |
-------------------- | ---------------- | --------------- | --------------- |
Llama 3.2-11B Vision  | MllamaForConditionalGeneration | âœ… | âœ… |
Llama 3.2-90B Vision  | MllamaForConditionalGeneration | âœ”ï¸ | âœ”ï¸ |
Granite 3.2-2B Vision  | LlavaNextForConditionalGeneration | âœ… | âœ… |
Llava Mistral 1.6-7B  | LlavaNextForConditionalGeneration | âœ… | âœ… |
Llava 1.6-34B  | LlavaNextForConditionalGeneration | âœ”ï¸ | âœ”ï¸ |
Llava 1.5-7B  | LlavaForConditionalGeneration | âœ… | âœ… |
Llava 1.5-13B  | LlavaForConditionalGeneration | âœ”ï¸ | âœ”ï¸ |

**Note**:
* vLLM currently does not support inference with LoRA-tuned vision models. To use a tuned LoRA adapter of vision model, please merge it with the base model before running vLLM inference.