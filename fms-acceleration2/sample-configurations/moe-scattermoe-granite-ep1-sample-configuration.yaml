# FMS Acceleration Plugin Configuration. 
#
# Each stanza incorporates various configurations for 
# different fine-tuning / training tasks.
plugins:
  training:

    # mixture-of-experts configurations
    moe:

      # expert-parallel for MoE
      scattermoe:

        # The level of expert parallel sharding. 
        # - 1 means no sharding
        # - if > 1, please ensure that this divides the world_size. This is because
        #   the devices will be replicated for every ep_degree devices, and 
        #   the experts will be sharded within each group.
        # - if > 1, also ensure that it divides the number of experts, as each device
        #   will then have num_of_experts / ep_degree experts.
        ep_degree: 1
